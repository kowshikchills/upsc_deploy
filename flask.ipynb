{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk import sent_tokenize\n",
    "nltk.download('punkt')\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from setfit import SetFitModel, SetFitTrainer\n",
    "from datasets import load_dataset\n",
    "from sentence_transformers.losses import CosineSimilarityLoss\n",
    "import datasets\n",
    "from datasets import Dataset, DatasetDict\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import trafilatura\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import (\n",
    "    TokenClassificationPipeline,\n",
    "    AutoModelForTokenClassification,\n",
    "    AutoTokenizer)\n",
    "from transformers.pipelines import AggregationStrategy\n",
    "import numpy as np\n",
    "import pickle\n",
    "import re\n",
    "import boto3\n",
    "import json\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Jean-Baptiste/camembert-ner\")\n",
    "model_ner = AutoModelForTokenClassification.from_pretrained(\"Jean-Baptiste/camembert-ner\")\n",
    "model_sim = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "nlp = pipeline('ner', model=model_ner, tokenizer=tokenizer, aggregation_strategy=\"simple\")\n",
    "model = SetFitModel.from_pretrained(\"kowshik/upsc-classification-model-v1\")\n",
    "model_name = \"ml6team/keyphrase-extraction-kbir-inspec\"\n",
    "map_ = {'agriculture': 0,'culture': 1,'defence': 2,'economy': 3,'environment': 4,'geography': 5,'governance': 6,\n",
    "'health': 7,'history': 8,'international relations': 9,'polity': 10,'science&technology': 11,'society': 12,'sports': 13}\n",
    "inv_map = {v: k for k, v in map_.items()}\n",
    "\n",
    "__TableName__ = 'prod1_app_data'\n",
    "client  = boto3.client('dynamodb',region_name = 'ap-south-1')\n",
    "DB  = boto3.resource('dynamodb',region_name = 'ap-south-1')\n",
    "table = DB.Table(__TableName__)\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "def get_summary(url,pid,text,summarizer,th=120):\n",
    "    if len(text) > 1200:\n",
    "        summary = summarizer(text, max_length= th, min_length=120, do_sample=False)[0]['summary_text']\n",
    "        flag = 3 + pid*10\n",
    "        data = summary\n",
    "        item_summary = create_item(url,flag, data)\n",
    "        response = table.put_item(Item  = item_summary)\n",
    "        return(summary)\n",
    "    else:\n",
    "        flag = 3 + pid*10\n",
    "        data = text\n",
    "        item_summary = create_item(url,flag, data)\n",
    "        response = table.put_item(Item  = item_summary)\n",
    "        return(text)\n",
    "\n",
    "f = open('rm_model.pkl', 'rb')\n",
    "clf = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "def create_item(url,flag, data):\n",
    "    '''  \n",
    "    flag = 0 > text\n",
    "    flag = 1 > sentence\n",
    "    flag = 2 > key Phrase \n",
    "    flag = 3 > summary \n",
    "    '''\n",
    "    item = {\n",
    "        'url': url,\n",
    "        'flag':flag,\n",
    "        'data': data,\n",
    "    }\n",
    "    return(item)\n",
    "\n",
    "\n",
    "def get_data_url(url):\n",
    "    downloaded = trafilatura.fetch_url(url)\n",
    "    text_original = trafilatura.extract(downloaded)\n",
    "    text_extracted = text_original.replace('\\n',' ')\n",
    "    flag = 0\n",
    "    data = text_original\n",
    "    item_complete = create_item(url,flag, data)\n",
    "    response = table.put_item(Item  = item_complete)\n",
    "    return(text_extracted, text_original)\n",
    "\n",
    "def get_label(word,model_sim):\n",
    "    labels = ['Environment','Geography','International Relations',\n",
    "    'Polity','Governance','Health','Society','Economy','Science&Technology','Agriculture','sports']\n",
    "    labels = [i.lower() for i in labels]\n",
    "    embeddings_tags = model_sim.encode(labels)\n",
    "    embeddings_key = model_sim.encode(word)\n",
    "    probs = cosine_similarity([embeddings_key],embeddings_tags)\n",
    "    label_index = np.argmax(probs)\n",
    "    return(labels[label_index])\n",
    "\n",
    "class KeyphraseExtractionPipeline(TokenClassificationPipeline):\n",
    "    def __init__(self, model, *args, **kwargs):\n",
    "        super().__init__(\n",
    "            model=AutoModelForTokenClassification.from_pretrained(model),\n",
    "            tokenizer=AutoTokenizer.from_pretrained(model),\n",
    "            *args,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "    def postprocess(self, model_outputs):\n",
    "        results = super().postprocess(\n",
    "            model_outputs=model_outputs,\n",
    "            aggregation_strategy=AggregationStrategy.SIMPLE,\n",
    "        )\n",
    "        return np.unique([result.get(\"word\").strip() for result in results])\n",
    "extractor = KeyphraseExtractionPipeline(model=model_name)\n",
    "\n",
    "def get_keywords_text(url,pid, sentences, extractor):\n",
    "    keywords_ = []\n",
    "    for te in sentences:\n",
    "        keywords_ = keywords_+ list(extractor(te))\n",
    "    keywords_unq = np.unique(keywords_)\n",
    "    flag = 2 +pid*10\n",
    "    data = json.dumps(list(keywords_unq))\n",
    "    item_key = create_item(url,  flag, data,)\n",
    "    response = table.put_item(Item  = item_key)\n",
    "    return(keywords_unq)\n",
    "\n",
    "alphabets= \"([A-Za-z])\"\n",
    "prefixes = \"(Mr|St|Mrs|Ms|Dr)[.]\"\n",
    "suffixes = \"(Inc|Ltd|Jr|Sr|Co)\"\n",
    "starters = \"(Mr|Mrs|Ms|Dr|He\\s|She\\s|It\\s|They\\s|Their\\s|Our\\s|We\\s|But\\s|However\\s|That\\s|This\\s|Wherever)\"\n",
    "acronyms = \"([A-Z][.][A-Z][.](?:[A-Z][.])?)\"\n",
    "websites = \"[.](com|net|org|io|gov)\"\n",
    "digits = \"([0-9])\"\n",
    "\n",
    "def split_into_sentences(text):\n",
    "    text = \" \" + text + \"  \"\n",
    "    text = text.replace(\"\\n\",\" \")\n",
    "    text = re.sub(prefixes,\"\\\\1<prd>\",text)\n",
    "    text = re.sub(websites,\"<prd>\\\\1\",text)\n",
    "    text = re.sub(digits + \"[.]\" + digits,\"\\\\1<prd>\\\\2\",text)\n",
    "    if \"...\" in text: text = text.replace(\"...\",\"<prd><prd><prd>\")\n",
    "    if \"Ph.D\" in text: text = text.replace(\"Ph.D.\",\"Ph<prd>D<prd>\")\n",
    "    text = re.sub(\"\\s\" + alphabets + \"[.] \",\" \\\\1<prd> \",text)\n",
    "    text = re.sub(acronyms+\" \"+starters,\"\\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\\\\3<prd>\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.] \"+starters,\" \\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.]\",\" \\\\1<prd>\",text)\n",
    "    text = re.sub(\" \" + alphabets + \"[.]\",\" \\\\1<prd>\",text)\n",
    "    if \"”\" in text: text = text.replace(\".”\",\"”.\")\n",
    "    if \"\\\"\" in text: text = text.replace(\".\\\"\",\"\\\".\")\n",
    "    if \"!\" in text: text = text.replace(\"!\\\"\",\"\\\"!\")\n",
    "    if \"?\" in text: text = text.replace(\"?\\\"\",\"\\\"?\")\n",
    "    text = text.replace(\".\",\".<stop>\")\n",
    "    text = text.replace(\"?\",\"?<stop>\")\n",
    "    text = text.replace(\"!\",\"!<stop>\")\n",
    "    text = text.replace(\"<prd>\",\".\")\n",
    "    sentences = text.split(\"<stop>\")\n",
    "    sentences = sentences[:-1]\n",
    "    sentences = [s.strip() for s in sentences]\n",
    "\n",
    "    sentences_final = []\n",
    "    for sent in sentences:\n",
    "        if len(sent)>= 120:\n",
    "            if ',' in sent:\n",
    "                pos_comma = np.array([i for i in range(len(sent)) if sent.startswith(',', i)])\n",
    "                to_split = np.argmin(np.abs(pos_comma - (len(sent) - pos_comma)))\n",
    "                if (pos_comma[to_split])<= 50 or (len(sent)-pos_comma[to_split] <= 50):\n",
    "                    sentences_final.append(sent)\n",
    "                else:\n",
    "                    sentences_final.append(sent[:pos_comma[to_split]])\n",
    "                    sentences_final.append(sent[pos_comma[to_split] +1:])\n",
    "            else:\n",
    "                sentences_final.append(sent)\n",
    "        else:\n",
    "            sentences_final.append(sent)\n",
    "\n",
    "    return sentences_final\n",
    "\n",
    "\n",
    "def get_sentence_labels(url, pid, sentences, clf, model, sent_no=4):\n",
    "    prediction_probas = clf.predict_proba(model.predict_proba(sentences))\n",
    "    df = pd.DataFrame()\n",
    "    df['sentences'] = sentences\n",
    "    df['labels_1'] = np.argmax(prediction_probas,axis=1)\n",
    "    df['prob_1'] = np.max(prediction_probas,axis=1)\n",
    "    df['label_text_1'] = df['labels_1'].replace(inv_map)\n",
    "    df['labels_2'] = [[list(p).index(i) for i in sorted(p, reverse=True)][1]  for p in prediction_probas]\n",
    "    df['prob_2'] = [p[[list(p).index(i) for i in sorted(p, reverse=True)][1]]  for p in prediction_probas]\n",
    "    df['label_text_2'] = df['labels_2'].replace(inv_map)\n",
    "    df = df.sort_values('prob_1',ascending=False)\n",
    "    labels = df[['sentences','label_text_1','label_text_2']][:sent_no]\n",
    "    flag = 1  + pid*10\n",
    "    data = json.dumps(labels.set_index('sentences').to_dict('index'))\n",
    "    item_sentence = create_item(url, flag, data)\n",
    "    response = table.put_item(Item  = item_sentence)\n",
    "\n",
    "    return(labels)\n",
    "\n",
    "\n",
    "\n",
    "def get_cuts(text, sentences_all):\n",
    "    if len(text) > 3000:\n",
    "        cumsum_ = np.cumsum([len(i) for i in sentences_all])\n",
    "        chunks = np.round(len(text)/2500)\n",
    "        cutoff_ = int(len(text)/chunks)\n",
    "        cuts = [0]\n",
    "        for i in np.arange(1,chunks):\n",
    "            cutoff = cutoff_*i\n",
    "            cut = np.argmin(np.abs(cumsum_ - cutoff))\n",
    "            cuts.append(cut)\n",
    "        cuts.append(len(sentences_all))\n",
    "        \n",
    "        sentences_chunks = []\n",
    "        for c in range(0,len(cuts)-1):\n",
    "            sentences_chunks.append(sentences_all[cuts[c]:cuts[c+1]])\n",
    "        return(sentences_chunks)\n",
    "    else:\n",
    "        return([sentences_all])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''  \n",
    "import boto3\n",
    "df = pd.read_pickle('/home/ubuntu/scrape_upsc_db/data/article_links.pkl')\n",
    "links = df[:100]\n",
    "items_list = []\n",
    "for i in links:\n",
    "    items = {\n",
    "            'user': 'kowshikchilamkurthy@gmail.com',\n",
    "            'url':i,\n",
    "        }\n",
    "    items_list.append(items)\n",
    "\n",
    "__TableName__ = 'prod1_user_data'\n",
    "client_user  = boto3.client('dynamodb',region_name = 'ap-south-1')\n",
    "DB_user  = boto3.resource('dynamodb',region_name = 'ap-south-1')\n",
    "table_user = DB_user.Table(__TableName__)\n",
    "for item_ in items_list:\n",
    "    response = table_user.put_item(Item  = item_)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.gktoday.in/topic/maharashtra-first-state-to-set-up-divyang-department/'\n",
    "text, text_act = get_data_url(url)\n",
    "if len(text) > 120:\n",
    "    sentences_all = split_into_sentences(text)\n",
    "    sentences_chunks = get_cuts(text, sentences_all)\n",
    "    pid = 0\n",
    "    for payload in sentences_chunks:\n",
    "        sentence_labels = get_sentence_labels(url,pid,payload,clf,model,sent_no=4)\n",
    "        sentence_keywords = list(set(payload) -  set(sentence_labels.sentences.values))\n",
    "        keyphrases = get_keywords_text(url,pid,sentence_keywords,extractor)\n",
    "        text = ' '.join(payload)\n",
    "        summary = get_summary(url,pid,text, summarizer, th= min(int(len(text)/10),240))\n",
    "        pid = pid+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_ = 'Policy Change Request Effective 05/29/2020 Please remove the PO Box from the insured’s mailing address.  They no longer have it.  The mailing address should read: 4 South Main Street Haydenville, MA  01039 All other aspects of the policy should remain unchanged.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor(str_, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in keyphrases:\n",
    "    print(i,';',inv_map[clf.predict(model.predict_proba([i]))[0]] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.predict_proba(model.predict_proba(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer(text[:5000], max_length= 240, min_length=120, do_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "json.dumps(list(keyphrases))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "__TableName__ = 'prod1_app_data'\n",
    "client  = boto3.client('dynamodb',region_name = 'ap-south-1')\n",
    "DB  = boto3.resource('dynamodb',region_name = 'ap-south-1')\n",
    "table = DB.Table(__TableName__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.hindustantimes.com/india-news/expect-russia-to-be-part-of-all-processes-says-india-on-g20-presidency-101669906231427.html'\n",
    "auth = '1'\n",
    "flag = 1\n",
    "text = sentence_labels[0][0]\n",
    "label_1 = sentence_labels[0][1]\n",
    "label_2 = sentence_labels[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "item = create_item(url,auth, flag,text, label_1,label_2)\n",
    "response = table.put_item(Item  = item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"label_text_2\":{\"M\":{\" the Global South, such as food, fuel and fertilisers,” he said.\":{\"S\":\"society\"},\"However, Prime Minister Narendra Modi told Russian President Vladimir Putin at a meeting in September that today’s era is “not of war”.\":{\"S\":\"international relations\"},\"From time to time, both countries indicate areas of interest or priority that they may be looking at”.\":{\"S\":\"geography\"},\"India said on Thursday it expects Russia to be part of all the processes of G20 as it assumed the presidency of the grouping against the backdrop of persisting differences among its members over the Ukraine war.\":{\"S\":\"defence\"},\"Expect Russia to be part of all processes, says India on G20 presidency India, which began its year-long G20 presidency on Thursday, and Indonesia\":{\"S\":\"geography\"}}},\"label_text_1\":{\"M\":{\" the Global South, such as food, fuel and fertilisers,” he said.\":{\"S\":\"agriculture\"},\"However, Prime Minister Narendra Modi told Russian President Vladimir Putin at a meeting in September that today’s era is “not of war”.\":{\"S\":\"defence\"},\"From time to time, both countries indicate areas of interest or priority that they may be looking at”.\":{\"S\":\"international relations\"},\"India said on Thursday it expects Russia to be part of all the processes of G20 as it assumed the presidency of the grouping against the backdrop of persisting differences among its members over the Ukraine war.\":{\"S\":\"international relations\"},\"Expect Russia to be part of all processes, says India on G20 presidency India, which began its year-long G20 presidency on Thursday, and Indonesia\":{\"S\":\"international relations\"}}}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model.predict_proba(['Expect Russia to be part of all processes, says India on G20 presidency India, which began its year-long G20 presidency on Thursday, and Indonesia'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict_proba(['The decisive moment will be September [2023] when the [G20] summit comes together.'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len('Expect Russia to be part of all processes, says India on G20 presidency India, which began its year-long G20 presidency on Thursday, and Indonesia, the previous president, played a key role in finalising a joint communique at the Bali summit amid deep divisions between Russia and the West')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len('Expect Russia to be part of all processes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([len(i) for i in sentences_final])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([len(i) for i in sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sentences_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s =  'Expect Russia to be part of all processes, says India on G20 presidency India which began its year-long G20 presidency on Thursday '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_comma = s.split(',', 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_comma = np.array([i for i in range(len(s)) if s.startswith(',', i)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmin(np.abs(pos_comma - (len(s) - pos_comma)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_comma[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s[:146]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s[147:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " (len(s) - pos_comma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions_model(text, model, map_):\n",
    "    predictions = model.predict_proba(text.split('.'))\n",
    "    predictions_label = []\n",
    "    for i in predictions:\n",
    "        if np.max(i)> th:\n",
    "            predictions_label.append(np.argmax(predictions[0])) \n",
    "        else:\n",
    "            predictions_label.append(None) \n",
    "    return(text.split('.'), predictions_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(['environment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "keywords = []\n",
    "for sent in text.split('.'):\n",
    "    for i in nlp(sent):\n",
    "        keywords.append(i['word'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "class TextRank4Keyword():\n",
    "    \"\"\"Extract keywords from text\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.d = 0.85 # damping coefficient, usually is .85\n",
    "        self.min_diff = 1e-5 # convergence threshold\n",
    "        self.steps = 10 # iteration steps\n",
    "        self.node_weight = None # save keywords and its weight\n",
    "\n",
    "    \n",
    "    def set_stopwords(self, stopwords):  \n",
    "        \"\"\"Set stop words\"\"\"\n",
    "        for word in STOP_WORDS.union(set(stopwords)):\n",
    "            lexeme = nlp.vocab[word]\n",
    "            lexeme.is_stop = True\n",
    "    \n",
    "    def sentence_segment(self, doc, candidate_pos, lower):\n",
    "        \"\"\"Store those words only in cadidate_pos\"\"\"\n",
    "        sentences = []\n",
    "        for sent in doc.sents:\n",
    "            selected_words = []\n",
    "            for token in sent:\n",
    "                # Store words only with cadidate POS tag\n",
    "                if token.pos_ in candidate_pos and token.is_stop is False:\n",
    "                    if lower is True:\n",
    "                        selected_words.append(token.text.lower())\n",
    "                    else:\n",
    "                        selected_words.append(token.text)\n",
    "            sentences.append(selected_words)\n",
    "        return sentences\n",
    "        \n",
    "    def get_vocab(self, sentences):\n",
    "        \"\"\"Get all tokens\"\"\"\n",
    "        vocab = OrderedDict()\n",
    "        i = 0\n",
    "        for sentence in sentences:\n",
    "            for word in sentence:\n",
    "                if word not in vocab:\n",
    "                    vocab[word] = i\n",
    "                    i += 1\n",
    "        return vocab\n",
    "    \n",
    "    def get_token_pairs(self, window_size, sentences):\n",
    "        \"\"\"Build token_pairs from windows in sentences\"\"\"\n",
    "        token_pairs = list()\n",
    "        for sentence in sentences:\n",
    "            for i, word in enumerate(sentence):\n",
    "                for j in range(i+1, i+window_size):\n",
    "                    if j >= len(sentence):\n",
    "                        break\n",
    "                    pair = (word, sentence[j])\n",
    "                    if pair not in token_pairs:\n",
    "                        token_pairs.append(pair)\n",
    "        return token_pairs\n",
    "        \n",
    "    def symmetrize(self, a):\n",
    "        return a + a.T - np.diag(a.diagonal())\n",
    "    \n",
    "    def get_matrix(self, vocab, token_pairs):\n",
    "        \"\"\"Get normalized matrix\"\"\"\n",
    "        # Build matrix\n",
    "        vocab_size = len(vocab)\n",
    "        g = np.zeros((vocab_size, vocab_size), dtype='float')\n",
    "        for word1, word2 in token_pairs:\n",
    "            i, j = vocab[word1], vocab[word2]\n",
    "            g[i][j] = 1\n",
    "            \n",
    "        # Get Symmeric matrix\n",
    "        g = self.symmetrize(g)\n",
    "        \n",
    "        # Normalize matrix by column\n",
    "        norm = np.sum(g, axis=0)\n",
    "        g_norm = np.divide(g, norm, where=norm!=0) # this is ignore the 0 element in norm\n",
    "        \n",
    "        return g_norm\n",
    "\n",
    "    \n",
    "    def get_keywords(self, number=10):\n",
    "        \"\"\"Print top number keywords\"\"\"\n",
    "        node_weight = OrderedDict(sorted(self.node_weight.items(), key=lambda t: t[1], reverse=True))\n",
    "        for i, (key, value) in enumerate(node_weight.items()):\n",
    "            print(key + ' - ' + str(value))\n",
    "            if i > number:\n",
    "                break\n",
    "        \n",
    "        \n",
    "    def analyze(self, text, \n",
    "                candidate_pos=['NOUN', 'PROPN'], \n",
    "                window_size=4, lower=False, stopwords=list()):\n",
    "        \"\"\"Main function to analyze text\"\"\"\n",
    "        \n",
    "        # Set stop words\n",
    "        self.set_stopwords(stopwords)\n",
    "        \n",
    "        # Pare text by spaCy\n",
    "        doc = nlp(text)\n",
    "        \n",
    "        # Filter sentences\n",
    "        sentences = self.sentence_segment(doc, candidate_pos, lower) # list of list of words\n",
    "        \n",
    "        # Build vocabulary\n",
    "        vocab = self.get_vocab(sentences)\n",
    "        \n",
    "        # Get token_pairs from windows\n",
    "        token_pairs = self.get_token_pairs(window_size, sentences)\n",
    "        \n",
    "        # Get normalized matrix\n",
    "        g = self.get_matrix(vocab, token_pairs)\n",
    "        \n",
    "        # Initionlization for weight(pagerank value)\n",
    "        pr = np.array([1] * len(vocab))\n",
    "        \n",
    "        # Iteration\n",
    "        previous_pr = 0\n",
    "        for epoch in range(self.steps):\n",
    "            pr = (1-self.d) + self.d * np.dot(g, pr)\n",
    "            if abs(previous_pr - sum(pr))  < self.min_diff:\n",
    "                break\n",
    "            else:\n",
    "                previous_pr = sum(pr)\n",
    "\n",
    "        # Get weight for each node\n",
    "        node_weight = dict()\n",
    "        for word, index in vocab.items():\n",
    "            node_weight[word] = pr[index]\n",
    "        \n",
    "        self.node_weight = node_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr4w = TextRank4Keyword()\n",
    "tr4w.analyze(text, candidate_pos = ['NOUN','PROPN', 'VERB'], window_size=10, lower=False)\n",
    "tr4w.get_keywords(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in nlp(df.headings.values[14]):\n",
    "    print(i['word'], get_label(i['word'],model_sim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_label('apple',model_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.headings.values[14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('data/training_data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install trafilatura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_extracted.replace('\\n',' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENTRYPOINT [\"uvicorn main:app --host\", \"0.0.0.0\", \"--port\", \"80\"]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('deployenv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9d43577eea919236b69ebd0f8998005d41bfb2309937fa01b8dc5c2f454a02cb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
