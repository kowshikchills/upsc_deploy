{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk import sent_tokenize\n",
    "nltk.download('punkt')\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from setfit import SetFitModel, SetFitTrainer\n",
    "from datasets import load_dataset\n",
    "from sentence_transformers.losses import CosineSimilarityLoss\n",
    "import datasets\n",
    "from datasets import Dataset, DatasetDict\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import trafilatura\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import (\n",
    "    TokenClassificationPipeline,\n",
    "    AutoModelForTokenClassification,\n",
    "    AutoTokenizer)\n",
    "from transformers.pipelines import AggregationStrategy\n",
    "import numpy as np\n",
    "import pickle\n",
    "import re\n",
    "import boto3\n",
    "import json\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Jean-Baptiste/camembert-ner\")\n",
    "model_ner = AutoModelForTokenClassification.from_pretrained(\"Jean-Baptiste/camembert-ner\")\n",
    "model_sim = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "nlp = pipeline('ner', model=model_ner, tokenizer=tokenizer, aggregation_strategy=\"simple\")\n",
    "model = SetFitModel.from_pretrained(\"kowshik/upsc-classification-model-v1\")\n",
    "model_name = \"ml6team/keyphrase-extraction-kbir-inspec\"\n",
    "map_ = {'agriculture': 0,'culture': 1,'defence': 2,'economy': 3,'environment': 4,'geography': 5,'governance': 6,\n",
    "'health': 7,'history': 8,'international relations': 9,'polity': 10,'science&technology': 11,'society': 12,'sports': 13}\n",
    "inv_map = {v: k for k, v in map_.items()}\n",
    "\n",
    "__TableName__ = 'prod1_app_data'\n",
    "client  = boto3.client('dynamodb',region_name = 'ap-south-1')\n",
    "DB  = boto3.resource('dynamodb',region_name = 'ap-south-1')\n",
    "table = DB.Table(__TableName__)\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "def get_summary(url,pid,text,summarizer,th=120):\n",
    "    if len(text) > 1200:\n",
    "        summary = summarizer(text, max_length= th, min_length=120, do_sample=False)[0]['summary_text']\n",
    "        flag = 3 + pid*10\n",
    "        data = summary\n",
    "        item_summary = create_item(url,flag, data)\n",
    "        response = table.put_item(Item  = item_summary)\n",
    "        return(summary)\n",
    "    else:\n",
    "        flag = 3 + pid*10\n",
    "        data = text\n",
    "        item_summary = create_item(url,flag, data)\n",
    "        response = table.put_item(Item  = item_summary)\n",
    "        return(text)\n",
    "\n",
    "f = open('rm_model.pkl', 'rb')\n",
    "clf = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "def create_item(url,flag, data):\n",
    "    '''  \n",
    "    flag = 0 > text\n",
    "    flag = 1 > sentence\n",
    "    flag = 2 > key Phrase \n",
    "    flag = 3 > summary \n",
    "    '''\n",
    "    item = {\n",
    "        'url': url,\n",
    "        'flag':flag,\n",
    "        'data': data,\n",
    "    }\n",
    "    return(item)\n",
    "\n",
    "\n",
    "def get_data_url(url):\n",
    "    downloaded = trafilatura.fetch_url(url)\n",
    "    text_original = trafilatura.extract(downloaded)\n",
    "    text_extracted = text_original.replace('\\n',' ')\n",
    "    flag = 0\n",
    "    data = text_original\n",
    "    item_complete = create_item(url,flag, data)\n",
    "    response = table.put_item(Item  = item_complete)\n",
    "    return(text_extracted, text_original)\n",
    "\n",
    "def get_label(word,model_sim):\n",
    "    labels = ['Environment','Geography','International Relations',\n",
    "    'Polity','Governance','Health','Society','Economy','Science&Technology','Agriculture','sports']\n",
    "    labels = [i.lower() for i in labels]\n",
    "    embeddings_tags = model_sim.encode(labels)\n",
    "    embeddings_key = model_sim.encode(word)\n",
    "    probs = cosine_similarity([embeddings_key],embeddings_tags)\n",
    "    label_index = np.argmax(probs)\n",
    "    return(labels[label_index])\n",
    "\n",
    "class KeyphraseExtractionPipeline(TokenClassificationPipeline):\n",
    "    def __init__(self, model, *args, **kwargs):\n",
    "        super().__init__(\n",
    "            model=AutoModelForTokenClassification.from_pretrained(model),\n",
    "            tokenizer=AutoTokenizer.from_pretrained(model),\n",
    "            *args,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "    def postprocess(self, model_outputs):\n",
    "        results = super().postprocess(\n",
    "            model_outputs=model_outputs,\n",
    "            aggregation_strategy=AggregationStrategy.SIMPLE,\n",
    "        )\n",
    "        return np.unique([result.get(\"word\").strip() for result in results])\n",
    "extractor = KeyphraseExtractionPipeline(model=model_name)\n",
    "\n",
    "def get_keywords_text(url,pid, sentences, extractor):\n",
    "    keywords_ = []\n",
    "    for te in sentences:\n",
    "        keywords_ = keywords_+ list(extractor(te))\n",
    "    keywords_unq = np.unique(keywords_)\n",
    "    flag = 2 +pid*10\n",
    "    data = json.dumps(list(keywords_unq))\n",
    "    item_key = create_item(url,  flag, data,)\n",
    "    response = table.put_item(Item  = item_key)\n",
    "    return(keywords_unq)\n",
    "\n",
    "alphabets= \"([A-Za-z])\"\n",
    "prefixes = \"(Mr|St|Mrs|Ms|Dr)[.]\"\n",
    "suffixes = \"(Inc|Ltd|Jr|Sr|Co)\"\n",
    "starters = \"(Mr|Mrs|Ms|Dr|He\\s|She\\s|It\\s|They\\s|Their\\s|Our\\s|We\\s|But\\s|However\\s|That\\s|This\\s|Wherever)\"\n",
    "acronyms = \"([A-Z][.][A-Z][.](?:[A-Z][.])?)\"\n",
    "websites = \"[.](com|net|org|io|gov)\"\n",
    "digits = \"([0-9])\"\n",
    "\n",
    "def split_into_sentences(text):\n",
    "    text = \" \" + text + \"  \"\n",
    "    text = text.replace(\"\\n\",\" \")\n",
    "    text = re.sub(prefixes,\"\\\\1<prd>\",text)\n",
    "    text = re.sub(websites,\"<prd>\\\\1\",text)\n",
    "    text = re.sub(digits + \"[.]\" + digits,\"\\\\1<prd>\\\\2\",text)\n",
    "    if \"...\" in text: text = text.replace(\"...\",\"<prd><prd><prd>\")\n",
    "    if \"Ph.D\" in text: text = text.replace(\"Ph.D.\",\"Ph<prd>D<prd>\")\n",
    "    text = re.sub(\"\\s\" + alphabets + \"[.] \",\" \\\\1<prd> \",text)\n",
    "    text = re.sub(acronyms+\" \"+starters,\"\\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\\\\3<prd>\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.] \"+starters,\" \\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.]\",\" \\\\1<prd>\",text)\n",
    "    text = re.sub(\" \" + alphabets + \"[.]\",\" \\\\1<prd>\",text)\n",
    "    if \"”\" in text: text = text.replace(\".”\",\"”.\")\n",
    "    if \"\\\"\" in text: text = text.replace(\".\\\"\",\"\\\".\")\n",
    "    if \"!\" in text: text = text.replace(\"!\\\"\",\"\\\"!\")\n",
    "    if \"?\" in text: text = text.replace(\"?\\\"\",\"\\\"?\")\n",
    "    text = text.replace(\".\",\".<stop>\")\n",
    "    text = text.replace(\"?\",\"?<stop>\")\n",
    "    text = text.replace(\"!\",\"!<stop>\")\n",
    "    text = text.replace(\"<prd>\",\".\")\n",
    "    sentences = text.split(\"<stop>\")\n",
    "    sentences = sentences[:-1]\n",
    "    sentences = [s.strip() for s in sentences]\n",
    "\n",
    "    sentences_final = []\n",
    "    for sent in sentences:\n",
    "        if len(sent)>= 120:\n",
    "            if ',' in sent:\n",
    "                pos_comma = np.array([i for i in range(len(sent)) if sent.startswith(',', i)])\n",
    "                to_split = np.argmin(np.abs(pos_comma - (len(sent) - pos_comma)))\n",
    "                if (pos_comma[to_split])<= 50 or (len(sent)-pos_comma[to_split] <= 50):\n",
    "                    sentences_final.append(sent)\n",
    "                else:\n",
    "                    sentences_final.append(sent[:pos_comma[to_split]])\n",
    "                    sentences_final.append(sent[pos_comma[to_split] +1:])\n",
    "            else:\n",
    "                sentences_final.append(sent)\n",
    "        else:\n",
    "            sentences_final.append(sent)\n",
    "\n",
    "    return sentences_final\n",
    "\n",
    "\n",
    "def get_sentence_labels(url, pid, sentences, clf, model, sent_no=4):\n",
    "    prediction_probas = clf.predict_proba(model.predict_proba(sentences))\n",
    "    df = pd.DataFrame()\n",
    "    df['sentences'] = sentences\n",
    "    df['labels_1'] = np.argmax(prediction_probas,axis=1)\n",
    "    df['prob_1'] = np.max(prediction_probas,axis=1)\n",
    "    df['label_text_1'] = df['labels_1'].replace(inv_map)\n",
    "    df['labels_2'] = [[list(p).index(i) for i in sorted(p, reverse=True)][1]  for p in prediction_probas]\n",
    "    df['prob_2'] = [p[[list(p).index(i) for i in sorted(p, reverse=True)][1]]  for p in prediction_probas]\n",
    "    df['label_text_2'] = df['labels_2'].replace(inv_map)\n",
    "    df = df.sort_values('prob_1',ascending=False)\n",
    "    labels = df[['sentences','label_text_1','label_text_2']][:sent_no]\n",
    "    flag = 1  + pid*10\n",
    "    data = json.dumps(labels.set_index('sentences').to_dict('index'))\n",
    "    item_sentence = create_item(url, flag, data)\n",
    "    response = table.put_item(Item  = item_sentence)\n",
    "\n",
    "    return(labels)\n",
    "\n",
    "\n",
    "\n",
    "def get_cuts(text, sentences_all):\n",
    "    if len(text) > 3000:\n",
    "        cumsum_ = np.cumsum([len(i) for i in sentences_all])\n",
    "        chunks = np.round(len(text)/2500)\n",
    "        cutoff_ = int(len(text)/chunks)\n",
    "        cuts = [0]\n",
    "        for i in np.arange(1,chunks):\n",
    "            cutoff = cutoff_*i\n",
    "            cut = np.argmin(np.abs(cumsum_ - cutoff))\n",
    "            cuts.append(cut)\n",
    "        cuts.append(len(sentences_all))\n",
    "        \n",
    "        sentences_chunks = []\n",
    "        for c in range(0,len(cuts)-1):\n",
    "            sentences_chunks.append(sentences_all[cuts[c]:cuts[c+1]])\n",
    "        return(sentences_chunks)\n",
    "    else:\n",
    "        return([sentences_all])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''  \n",
    "import boto3\n",
    "df = pd.read_pickle('/home/ubuntu/scrape_upsc_db/data/article_links.pkl')\n",
    "links = df[:100]\n",
    "items_list = []\n",
    "for i in links:\n",
    "    items = {\n",
    "            'user': 'kowshikchilamkurthy@gmail.com',\n",
    "            'url':i,\n",
    "        }\n",
    "    items_list.append(items)\n",
    "\n",
    "__TableName__ = 'prod1_user_data'\n",
    "client_user  = boto3.client('dynamodb',region_name = 'ap-south-1')\n",
    "DB_user  = boto3.resource('dynamodb',region_name = 'ap-south-1')\n",
    "table_user = DB_user.Table(__TableName__)\n",
    "for item_ in items_list:\n",
    "    response = table_user.put_item(Item  = item_)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.thehindu.com/news/national/all-party-meet-ahead-of-budget-session-opposition-raises-adani-issue-ysr-congress-calls-for-caste-based-economic-census/article66450221.ec'\n",
    "text, text_act = get_data_url(url)\n",
    "if len(text) > 120:\n",
    "    sentences_all = split_into_sentences(text)\n",
    "    sentences_chunks = get_cuts(text, sentences_all)\n",
    "    pid = 0\n",
    "    for payload in sentences_chunks:\n",
    "        sentence_labels = get_sentence_labels(url,pid,payload,clf,model,sent_no=4)\n",
    "        sentence_keywords = list(set(payload) -  set(sentence_labels.sentences.values))\n",
    "        keyphrases = get_keywords_text(url,pid,sentence_keywords,extractor)\n",
    "        text = ' '.join(payload)\n",
    "        summary = get_summary(url,pid,text, summarizer, th= min(int(len(text)/10),240))\n",
    "        pid = pid+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from boto3.dynamodb.conditions import Key\n",
    "__TableName__ = 'prod1_app_data'\n",
    "client  = boto3.client('dynamodb',region_name = 'ap-south-1')\n",
    "DB  = boto3.resource('dynamodb',region_name = 'ap-south-1')\n",
    "table = DB.Table(__TableName__)\n",
    "response = table.query(\n",
    "  KeyConditionExpression=Key('url').eq('https://www.gktoday.in/topic/pusa-krishi-vigyan-mela/')\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Items': [{'flag': Decimal('0'),\n",
       "   'url': 'https://www.gktoday.in/topic/pusa-krishi-vigyan-mela/',\n",
       "   'data': 'Pusa Krishi Vigyan Mela\\nThe Union Minister for Agriculture and Farmers Welfare, Shri Narendra Singh Tomar, inaugurated the Pusa Krishi Vigyan Mela. The event is being organized by the Indian Agricultural Research Institute (IARI) in New Delhi. This year, the event is being organized under the theme of “Nutrition, Food and Environmental Protection with Shree Anna”.\\nContents\\nKey Highlights of Pusa Krishi Vigyan Mela\\nThe three-day event includes a fair that features live demonstrations of various crops and technologies, as well as stalls for input agencies and farmer counselling. Its aim is to empower small farmers, scientists and startups by solving major issues concerning the agriculture sector. During the event, awards were conferred to recognize progressive dedicated farmers.\\nWhat is the need for Pusa Krishi Vigyan Mela?\\nWith the climate change impacts increasing exponentially, there is a need for resilient agriculture. Only technologies can bring in climate-resilient agriculture. Take the recently launched HD-3385 new wheat variety for instance. The crop is climate resilient. It helps farmers to save their crops from high temperatures before harvest. Such agricultural advancements should be reached the farmers as the earliest. The Pusa Krishi Vigyan Mela will help farmers learn about the advancements and use of technologies.\\nWhy should GoI organize more such Melas?\\nToday, the top three countries in terms of Agricultural technologies are China, Japan, and Netherlands. China has deployed 18 UAV agriculture zones. With this, the country saves 30% of pesticide costs and 50% of labour costs. The Netherlands creates 90,000 tonnes of livestock feed from food waste. This aids in a sustainable production cycle. Japan uses cloud computing to monitor plantations. India should adopt its technologies more. With a huge population, festivals are the best way to reach many farmers within a short span. Today India is focusing on bringing investors in food processing. It should also focus on bringing in investors in agricultural technologies.\\nMonth: Current Affairs - March, 2023\\nCategory: Agriculture Current Affairs • Economy & Banking Current Affairs - 2022 • Events Current Affairs\\nTopics: Agriculture • Agriculture Technology • farmer welfare\\nLatest E-Books\\nCurrent Affairs MCQs PDF - February, 2023\\n|₹150.00|\\nCurrent Affairs Articles Compilation [PDF] - February, 2023\\n|₹200.00|\\nCurrent Affairs [PDF] - February 16-28, 2023\\n|₹150.00|\\n|View All E-Books: Recent Release|\\nComments'},\n",
       "  {'flag': Decimal('1'),\n",
       "   'url': 'https://www.gktoday.in/topic/pusa-krishi-vigyan-mela/',\n",
       "   'data': '{\"Pusa Krishi Vigyan Mela The Union Minister for Agriculture and Farmers Welfare\": {\"label_text_1\": \"agriculture\", \"label_text_2\": \"environment\"}, \"Take the recently launched HD-3385 new wheat variety for instance.\": {\"label_text_1\": \"agriculture\", \"label_text_2\": \"environment\"}, \"Today India is focusing on bringing investors in food processing.\": {\"label_text_1\": \"agriculture\", \"label_text_2\": \"environment\"}, \"With a huge population, festivals are the best way to reach many farmers within a short span.\": {\"label_text_1\": \"agriculture\", \"label_text_2\": \"environment\"}}'},\n",
       "  {'flag': Decimal('2'),\n",
       "   'url': 'https://www.gktoday.in/topic/pusa-krishi-vigyan-mela/',\n",
       "   'data': '[\"ARI\", \"China\", \"India\", \"Indian Agricultural Research Institute\", \"Japan\", \"Melas\", \"Netherlands\", \"Nutrition\", \"Pusa Krishi Vigyan Mela\", \"UAV agriculture zones\", \"agricultural technologies\", \"agriculture sector\", \"cloud computing\", \"farmer counselling\", \"farmers\", \"food waste\", \"input agencies\", \"livestock feed\", \"progressive dedicated farmers\", \"resilient agriculture\", \"small farmers\", \"startups\", \"sustainable production cycle\"]'},\n",
       "  {'flag': Decimal('3'),\n",
       "   'url': 'https://www.gktoday.in/topic/pusa-krishi-vigyan-mela/',\n",
       "   'data': 'The three-day event includes a fair that features live demonstrations of various crops and technologies. Its aim is to empower small farmers, scientists and startups by solving major issues concerning the agriculture sector. The top three countries in terms of Agricultural technologies are China, Japan, and Netherlands. China has deployed 18 UAV agriculture zones. The Netherlands creates 90,000 tonnes of livestock feed from food waste. Japan uses cloud computing to monitor plantations. India should adopt its technologies more. With a huge population, festivals are the best way to reach many farmers within a short span. The Pusa Krishi Vigyan Mela will help farmers learn about the advancements and use of technologies.'}],\n",
       " 'Count': 4,\n",
       " 'ScannedCount': 4,\n",
       " 'ResponseMetadata': {'RequestId': 'QQ2FPTN6BHO0KHGU5GTQBJ5TPBVV4KQNSO5AEMVJF66Q9ASUAAJG',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'server': 'Server',\n",
       "   'date': 'Wed, 08 Mar 2023 16:15:37 GMT',\n",
       "   'content-type': 'application/x-amz-json-1.0',\n",
       "   'content-length': '4814',\n",
       "   'connection': 'keep-alive',\n",
       "   'x-amzn-requestid': 'QQ2FPTN6BHO0KHGU5GTQBJ5TPBVV4KQNSO5AEMVJF66Q9ASUAAJG',\n",
       "   'x-amz-crc32': '3285479113'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_ = 'Policy Change Request Effective 05/29/2020 Please remove the PO Box from the insured’s mailing address.  They no longer have it.  The mailing address should read: 4 South Main Street Haydenville, MA  01039 All other aspects of the policy should remain unchanged.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor(str_, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in keyphrases:\n",
    "    print(i,';',inv_map[clf.predict(model.predict_proba([i]))[0]] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.predict_proba(model.predict_proba(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer(text[:5000], max_length= 240, min_length=120, do_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "json.dumps(list(keyphrases))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Items': [{'flag': Decimal('0'),\n",
       "   'url': 'https://www.gktoday.in/topic/what-is-millets-giveaway/',\n",
       "   'data': 'What is Millets Giveaway?\\nThe Small Farmers’ Agri-Business Consortium, a society promoted by the Ministry of Agriculture and Farmers Welfare of the Indian government, has launched a special marketing campaign called Millets Giveaway. The campaign is aimed at promoting direct purchases from small and marginal farmer producer organisations (FPOs) to support their livelihoods.\\nWhat is Millets Giveaway?\\nThe Millets Giveaway campaign encourages citizens to purchase millets directly from FPOs through ONDC’s My Store, a marketplace built for Indian sellers. Millets are the primary focus of the campaign, motivating more people to adopt these grains in their diet. This is a part of the nationwide and also the worldwide drive to promote these nutri-cereals in the International Year of Millets 2023.\\nSignificance of FPO in the campaign\\nThe campaign is being held through the FPOs. The FPOs have always played big roles in bringing changes in the lives of the farmers, especially the small and marginal farmers. Earlier the farmers used to cultivate the crops using traditional methods. FPOs helped them to learn new and conservative farming methods. Today, drip irrigation is a big success mainly because of these FPOs. Through good agricultural practices, the FPO helped farmers increase productivity by 30% to 50%. Also, the FPO played a large role in farm mechanisation. They rented farm machinery and demonstrated the usage of many machines.\\nWhy Giveaway campaign?\\nThe Government of India has been promoting millet on a large scale. During the Budget 2023-24, the finance minister Smt Nirmala Sitaraman announced that millets will be promoted under the brand name “Shree Anna”. The APEDA signed a pact with a private firm called LULU to promote millets in the GCC. LULU is into the supermarket business. The food processing ministry launched the Bhojpur Millet festival. The government is promoting millet because these crops are highly nutritious and climate-resilient. Also, the Millet International Initiative for Research and Awareness was launched. The campaign will help the government achieve its plan on promoting millet.\\nMonth: Current Affairs - March, 2023\\nCategory: Agriculture Current Affairs\\nTopics: Farmer Producer Organisation • farmer welfare • Millets\\nLatest E-Books\\nCurrent Affairs MCQs PDF - February, 2023\\n|₹150.00|\\nCurrent Affairs Articles Compilation [PDF] - February, 2023\\n|₹200.00|\\nCurrent Affairs [PDF] - February 16-28, 2023\\n|₹150.00|\\n|View All E-Books: Recent Release|\\nComments'},\n",
       "  {'flag': Decimal('1'),\n",
       "   'url': 'https://www.gktoday.in/topic/what-is-millets-giveaway/',\n",
       "   'data': '{\"LULU is into the supermarket business.\": {\"label_text_1\": \"agriculture\", \"label_text_2\": \"environment\"}, \"Also, the Millet International Initiative for Research and Awareness was launched.\": {\"label_text_1\": \"environment\", \"label_text_2\": \"geography\"}, \"The campaign will help the government achieve its plan on promoting millet.\": {\"label_text_1\": \"agriculture\", \"label_text_2\": \"environment\"}, \"During the Budget 2023-24, the finance minister Smt Nirmala Sitaraman announced that millets will be promoted under the brand name \\\\u201cShree Anna\\\\u201d.\": {\"label_text_1\": \"environment\", \"label_text_2\": \"geography\"}}'},\n",
       "  {'flag': Decimal('2'),\n",
       "   'url': 'https://www.gktoday.in/topic/what-is-millets-giveaway/',\n",
       "   'data': '[\"APEDA\", \"Bhojpur Millet festival\", \"F\", \"FPO\", \"Farmers Welfare\", \"GCC\", \"India\", \"Indian government\", \"Indian sellers\", \"LULU\", \"Millets\", \"Millets Giveaway\", \"agricultural practices\", \"conservative farming methods\", \"direct purchases\", \"drip irrigation\", \"farm machinery\", \"farm mechanisation\", \"farmers\", \"food processing ministry\", \"grains\", \"marginal farmer producer organisations\", \"marginal farmers\", \"marketing campaign\", \"millet\", \"millets\"]'},\n",
       "  {'flag': Decimal('3'),\n",
       "   'url': 'https://www.gktoday.in/topic/what-is-millets-giveaway/',\n",
       "   'data': 'The Small Farmers’ Agri-Business Consortium, a society promoted by the Ministry of Agriculture and Farmers Welfare of the Indian government has launched a special marketing campaign called Millets Giveaway. The campaign is aimed at promoting direct purchases from small and marginal farmer producer organisations (FPOs) to support their livelihoods. Millets are the primary focus of the campaign, motivating more people to adopt these grains in their diet. This is a part of the nationwide and also the worldwide drive to promote these nutri-cereals in the International Year of Millets 2023. The government is promoting millet because these crops are highly nutritious and climate-resilient.'}],\n",
       " 'Count': 4,\n",
       " 'ScannedCount': 4,\n",
       " 'ResponseMetadata': {'RequestId': '0VP8EE9B437TKH01UNAJEECVHRVV4KQNSO5AEMVJF66Q9ASUAAJG',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'server': 'Server',\n",
       "   'date': 'Wed, 08 Mar 2023 15:10:37 GMT',\n",
       "   'content-type': 'application/x-amz-json-1.0',\n",
       "   'content-length': '4856',\n",
       "   'connection': 'keep-alive',\n",
       "   'x-amzn-requestid': '0VP8EE9B437TKH01UNAJEECVHRVV4KQNSO5AEMVJF66Q9ASUAAJG',\n",
       "   'x-amz-crc32': '1089202407'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.gktoday.in/topic/what-is-millets-giveaway/'\n",
    "auth = '1'\n",
    "flag = 1\n",
    "text = sentence_labels[0][0]\n",
    "label_1 = sentence_labels[0][1]\n",
    "label_2 = sentence_labels[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "item = create_item(url,auth, flag,text, label_1,label_2)\n",
    "response = table.put_item(Item  = item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"label_text_2\":{\"M\":{\" the Global South, such as food, fuel and fertilisers,” he said.\":{\"S\":\"society\"},\"However, Prime Minister Narendra Modi told Russian President Vladimir Putin at a meeting in September that today’s era is “not of war”.\":{\"S\":\"international relations\"},\"From time to time, both countries indicate areas of interest or priority that they may be looking at”.\":{\"S\":\"geography\"},\"India said on Thursday it expects Russia to be part of all the processes of G20 as it assumed the presidency of the grouping against the backdrop of persisting differences among its members over the Ukraine war.\":{\"S\":\"defence\"},\"Expect Russia to be part of all processes, says India on G20 presidency India, which began its year-long G20 presidency on Thursday, and Indonesia\":{\"S\":\"geography\"}}},\"label_text_1\":{\"M\":{\" the Global South, such as food, fuel and fertilisers,” he said.\":{\"S\":\"agriculture\"},\"However, Prime Minister Narendra Modi told Russian President Vladimir Putin at a meeting in September that today’s era is “not of war”.\":{\"S\":\"defence\"},\"From time to time, both countries indicate areas of interest or priority that they may be looking at”.\":{\"S\":\"international relations\"},\"India said on Thursday it expects Russia to be part of all the processes of G20 as it assumed the presidency of the grouping against the backdrop of persisting differences among its members over the Ukraine war.\":{\"S\":\"international relations\"},\"Expect Russia to be part of all processes, says India on G20 presidency India, which began its year-long G20 presidency on Thursday, and Indonesia\":{\"S\":\"international relations\"}}}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model.predict_proba(['Expect Russia to be part of all processes, says India on G20 presidency India, which began its year-long G20 presidency on Thursday, and Indonesia'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict_proba(['The decisive moment will be September [2023] when the [G20] summit comes together.'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len('Expect Russia to be part of all processes, says India on G20 presidency India, which began its year-long G20 presidency on Thursday, and Indonesia, the previous president, played a key role in finalising a joint communique at the Bali summit amid deep divisions between Russia and the West')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len('Expect Russia to be part of all processes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([len(i) for i in sentences_final])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([len(i) for i in sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sentences_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s =  'Expect Russia to be part of all processes, says India on G20 presidency India which began its year-long G20 presidency on Thursday '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_comma = s.split(',', 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_comma = np.array([i for i in range(len(s)) if s.startswith(',', i)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmin(np.abs(pos_comma - (len(s) - pos_comma)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_comma[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s[:146]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s[147:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " (len(s) - pos_comma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions_model(text, model, map_):\n",
    "    predictions = model.predict_proba(text.split('.'))\n",
    "    predictions_label = []\n",
    "    for i in predictions:\n",
    "        if np.max(i)> th:\n",
    "            predictions_label.append(np.argmax(predictions[0])) \n",
    "        else:\n",
    "            predictions_label.append(None) \n",
    "    return(text.split('.'), predictions_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(['environment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "keywords = []\n",
    "for sent in text.split('.'):\n",
    "    for i in nlp(sent):\n",
    "        keywords.append(i['word'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "class TextRank4Keyword():\n",
    "    \"\"\"Extract keywords from text\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.d = 0.85 # damping coefficient, usually is .85\n",
    "        self.min_diff = 1e-5 # convergence threshold\n",
    "        self.steps = 10 # iteration steps\n",
    "        self.node_weight = None # save keywords and its weight\n",
    "\n",
    "    \n",
    "    def set_stopwords(self, stopwords):  \n",
    "        \"\"\"Set stop words\"\"\"\n",
    "        for word in STOP_WORDS.union(set(stopwords)):\n",
    "            lexeme = nlp.vocab[word]\n",
    "            lexeme.is_stop = True\n",
    "    \n",
    "    def sentence_segment(self, doc, candidate_pos, lower):\n",
    "        \"\"\"Store those words only in cadidate_pos\"\"\"\n",
    "        sentences = []\n",
    "        for sent in doc.sents:\n",
    "            selected_words = []\n",
    "            for token in sent:\n",
    "                # Store words only with cadidate POS tag\n",
    "                if token.pos_ in candidate_pos and token.is_stop is False:\n",
    "                    if lower is True:\n",
    "                        selected_words.append(token.text.lower())\n",
    "                    else:\n",
    "                        selected_words.append(token.text)\n",
    "            sentences.append(selected_words)\n",
    "        return sentences\n",
    "        \n",
    "    def get_vocab(self, sentences):\n",
    "        \"\"\"Get all tokens\"\"\"\n",
    "        vocab = OrderedDict()\n",
    "        i = 0\n",
    "        for sentence in sentences:\n",
    "            for word in sentence:\n",
    "                if word not in vocab:\n",
    "                    vocab[word] = i\n",
    "                    i += 1\n",
    "        return vocab\n",
    "    \n",
    "    def get_token_pairs(self, window_size, sentences):\n",
    "        \"\"\"Build token_pairs from windows in sentences\"\"\"\n",
    "        token_pairs = list()\n",
    "        for sentence in sentences:\n",
    "            for i, word in enumerate(sentence):\n",
    "                for j in range(i+1, i+window_size):\n",
    "                    if j >= len(sentence):\n",
    "                        break\n",
    "                    pair = (word, sentence[j])\n",
    "                    if pair not in token_pairs:\n",
    "                        token_pairs.append(pair)\n",
    "        return token_pairs\n",
    "        \n",
    "    def symmetrize(self, a):\n",
    "        return a + a.T - np.diag(a.diagonal())\n",
    "    \n",
    "    def get_matrix(self, vocab, token_pairs):\n",
    "        \"\"\"Get normalized matrix\"\"\"\n",
    "        # Build matrix\n",
    "        vocab_size = len(vocab)\n",
    "        g = np.zeros((vocab_size, vocab_size), dtype='float')\n",
    "        for word1, word2 in token_pairs:\n",
    "            i, j = vocab[word1], vocab[word2]\n",
    "            g[i][j] = 1\n",
    "            \n",
    "        # Get Symmeric matrix\n",
    "        g = self.symmetrize(g)\n",
    "        \n",
    "        # Normalize matrix by column\n",
    "        norm = np.sum(g, axis=0)\n",
    "        g_norm = np.divide(g, norm, where=norm!=0) # this is ignore the 0 element in norm\n",
    "        \n",
    "        return g_norm\n",
    "\n",
    "    \n",
    "    def get_keywords(self, number=10):\n",
    "        \"\"\"Print top number keywords\"\"\"\n",
    "        node_weight = OrderedDict(sorted(self.node_weight.items(), key=lambda t: t[1], reverse=True))\n",
    "        for i, (key, value) in enumerate(node_weight.items()):\n",
    "            print(key + ' - ' + str(value))\n",
    "            if i > number:\n",
    "                break\n",
    "        \n",
    "        \n",
    "    def analyze(self, text, \n",
    "                candidate_pos=['NOUN', 'PROPN'], \n",
    "                window_size=4, lower=False, stopwords=list()):\n",
    "        \"\"\"Main function to analyze text\"\"\"\n",
    "        \n",
    "        # Set stop words\n",
    "        self.set_stopwords(stopwords)\n",
    "        \n",
    "        # Pare text by spaCy\n",
    "        doc = nlp(text)\n",
    "        \n",
    "        # Filter sentences\n",
    "        sentences = self.sentence_segment(doc, candidate_pos, lower) # list of list of words\n",
    "        \n",
    "        # Build vocabulary\n",
    "        vocab = self.get_vocab(sentences)\n",
    "        \n",
    "        # Get token_pairs from windows\n",
    "        token_pairs = self.get_token_pairs(window_size, sentences)\n",
    "        \n",
    "        # Get normalized matrix\n",
    "        g = self.get_matrix(vocab, token_pairs)\n",
    "        \n",
    "        # Initionlization for weight(pagerank value)\n",
    "        pr = np.array([1] * len(vocab))\n",
    "        \n",
    "        # Iteration\n",
    "        previous_pr = 0\n",
    "        for epoch in range(self.steps):\n",
    "            pr = (1-self.d) + self.d * np.dot(g, pr)\n",
    "            if abs(previous_pr - sum(pr))  < self.min_diff:\n",
    "                break\n",
    "            else:\n",
    "                previous_pr = sum(pr)\n",
    "\n",
    "        # Get weight for each node\n",
    "        node_weight = dict()\n",
    "        for word, index in vocab.items():\n",
    "            node_weight[word] = pr[index]\n",
    "        \n",
    "        self.node_weight = node_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr4w = TextRank4Keyword()\n",
    "tr4w.analyze(text, candidate_pos = ['NOUN','PROPN', 'VERB'], window_size=10, lower=False)\n",
    "tr4w.get_keywords(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in nlp(df.headings.values[14]):\n",
    "    print(i['word'], get_label(i['word'],model_sim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_label('apple',model_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.headings.values[14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('data/training_data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install trafilatura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_extracted.replace('\\n',' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENTRYPOINT [\"uvicorn main:app --host\", \"0.0.0.0\", \"--port\", \"80\"]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('deployenv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9d43577eea919236b69ebd0f8998005d41bfb2309937fa01b8dc5c2f454a02cb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
