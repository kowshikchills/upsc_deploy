{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "/home/ubuntu/upsc_models_deployment/deployenv/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/ubuntu/upsc_models_deployment/deployenv/lib/python3.10/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator LogisticRegression from version 1.0.1 when using version 1.1.3. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk import sent_tokenize\n",
    "nltk.download('punkt')\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from setfit import SetFitModel, SetFitTrainer\n",
    "from datasets import load_dataset\n",
    "from sentence_transformers.losses import CosineSimilarityLoss\n",
    "import datasets\n",
    "from datasets import Dataset, DatasetDict\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import trafilatura\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import (\n",
    "    TokenClassificationPipeline,\n",
    "    AutoModelForTokenClassification,\n",
    "    AutoTokenizer)\n",
    "from transformers.pipelines import AggregationStrategy\n",
    "import numpy as np\n",
    "import pickle\n",
    "import re\n",
    "import boto3\n",
    "import json\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Jean-Baptiste/camembert-ner\")\n",
    "model_ner = AutoModelForTokenClassification.from_pretrained(\"Jean-Baptiste/camembert-ner\")\n",
    "model_sim = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "nlp = pipeline('ner', model=model_ner, tokenizer=tokenizer, aggregation_strategy=\"simple\")\n",
    "model = SetFitModel.from_pretrained(\"kowshik/upsc-classification-model-v1\")\n",
    "model_name = \"ml6team/keyphrase-extraction-kbir-inspec\"\n",
    "map_ = {'agriculture': 0,'culture': 1,'defence': 2,'economy': 3,'environment': 4,'geography': 5,'governance': 6,\n",
    "'health': 7,'history': 8,'international relations': 9,'polity': 10,'science&technology': 11,'society': 12,'sports': 13}\n",
    "inv_map = {v: k for k, v in map_.items()}\n",
    "\n",
    "__TableName__ = 'prod1_app_data'\n",
    "client  = boto3.client('dynamodb',region_name = 'ap-south-1')\n",
    "DB  = boto3.resource('dynamodb',region_name = 'ap-south-1')\n",
    "table = DB.Table(__TableName__)\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "def get_summary(url,pid,text,summarizer,th=120):\n",
    "    if len(text) > 1200:\n",
    "        summary = summarizer(text, max_length= th, min_length=120, do_sample=False)[0]['summary_text']\n",
    "        flag = 3 + pid*10\n",
    "        data = summary\n",
    "        item_summary = create_item(url,flag, data)\n",
    "        response = table.put_item(Item  = item_summary)\n",
    "        return(summary)\n",
    "    else:\n",
    "        flag = 3 + pid*10\n",
    "        data = text\n",
    "        item_summary = create_item(url,flag, data)\n",
    "        response = table.put_item(Item  = item_summary)\n",
    "        return(text)\n",
    "\n",
    "f = open('rm_model.pkl', 'rb')\n",
    "clf = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "def create_item(url,flag, data):\n",
    "    '''  \n",
    "    flag = 0 > text\n",
    "    flag = 1 > sentence\n",
    "    flag = 2 > key Phrase \n",
    "    flag = 3 > summary \n",
    "    '''\n",
    "    item = {\n",
    "        'url': url,\n",
    "        'flag':flag,\n",
    "        'data': data,\n",
    "    }\n",
    "    return(item)\n",
    "\n",
    "\n",
    "def get_data_url(url):\n",
    "    downloaded = trafilatura.fetch_url(url)\n",
    "    text_original = trafilatura.extract(downloaded)\n",
    "    text_extracted = text_original.replace('\\n',' ')\n",
    "    flag = 0\n",
    "    data = text_original\n",
    "    item_complete = create_item(url,flag, data)\n",
    "    response = table.put_item(Item  = item_complete)\n",
    "    return(text_extracted, text_original)\n",
    "\n",
    "def get_label(word,model_sim):\n",
    "    labels = ['Environment','Geography','International Relations',\n",
    "    'Polity','Governance','Health','Society','Economy','Science&Technology','Agriculture','sports']\n",
    "    labels = [i.lower() for i in labels]\n",
    "    embeddings_tags = model_sim.encode(labels)\n",
    "    embeddings_key = model_sim.encode(word)\n",
    "    probs = cosine_similarity([embeddings_key],embeddings_tags)\n",
    "    label_index = np.argmax(probs)\n",
    "    return(labels[label_index])\n",
    "\n",
    "class KeyphraseExtractionPipeline(TokenClassificationPipeline):\n",
    "    def __init__(self, model, *args, **kwargs):\n",
    "        super().__init__(\n",
    "            model=AutoModelForTokenClassification.from_pretrained(model),\n",
    "            tokenizer=AutoTokenizer.from_pretrained(model),\n",
    "            *args,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "    def postprocess(self, model_outputs):\n",
    "        results = super().postprocess(\n",
    "            model_outputs=model_outputs,\n",
    "            aggregation_strategy=AggregationStrategy.SIMPLE,\n",
    "        )\n",
    "        return np.unique([result.get(\"word\").strip() for result in results])\n",
    "extractor = KeyphraseExtractionPipeline(model=model_name)\n",
    "\n",
    "def get_keywords_text(url,pid, sentences, extractor):\n",
    "    keywords_ = []\n",
    "    for te in sentences:\n",
    "        keywords_ = keywords_+ list(extractor(te))\n",
    "    keywords_unq = np.unique(keywords_)\n",
    "    flag = 2 +pid*10\n",
    "    data = json.dumps(list(keywords_unq))\n",
    "    item_key = create_item(url,  flag, data,)\n",
    "    response = table.put_item(Item  = item_key)\n",
    "    return(keywords_unq)\n",
    "\n",
    "alphabets= \"([A-Za-z])\"\n",
    "prefixes = \"(Mr|St|Mrs|Ms|Dr)[.]\"\n",
    "suffixes = \"(Inc|Ltd|Jr|Sr|Co)\"\n",
    "starters = \"(Mr|Mrs|Ms|Dr|He\\s|She\\s|It\\s|They\\s|Their\\s|Our\\s|We\\s|But\\s|However\\s|That\\s|This\\s|Wherever)\"\n",
    "acronyms = \"([A-Z][.][A-Z][.](?:[A-Z][.])?)\"\n",
    "websites = \"[.](com|net|org|io|gov)\"\n",
    "digits = \"([0-9])\"\n",
    "\n",
    "def split_into_sentences(text):\n",
    "    text = \" \" + text + \"  \"\n",
    "    text = text.replace(\"\\n\",\" \")\n",
    "    text = re.sub(prefixes,\"\\\\1<prd>\",text)\n",
    "    text = re.sub(websites,\"<prd>\\\\1\",text)\n",
    "    text = re.sub(digits + \"[.]\" + digits,\"\\\\1<prd>\\\\2\",text)\n",
    "    if \"...\" in text: text = text.replace(\"...\",\"<prd><prd><prd>\")\n",
    "    if \"Ph.D\" in text: text = text.replace(\"Ph.D.\",\"Ph<prd>D<prd>\")\n",
    "    text = re.sub(\"\\s\" + alphabets + \"[.] \",\" \\\\1<prd> \",text)\n",
    "    text = re.sub(acronyms+\" \"+starters,\"\\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\\\\3<prd>\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.] \"+starters,\" \\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.]\",\" \\\\1<prd>\",text)\n",
    "    text = re.sub(\" \" + alphabets + \"[.]\",\" \\\\1<prd>\",text)\n",
    "    if \"”\" in text: text = text.replace(\".”\",\"”.\")\n",
    "    if \"\\\"\" in text: text = text.replace(\".\\\"\",\"\\\".\")\n",
    "    if \"!\" in text: text = text.replace(\"!\\\"\",\"\\\"!\")\n",
    "    if \"?\" in text: text = text.replace(\"?\\\"\",\"\\\"?\")\n",
    "    text = text.replace(\".\",\".<stop>\")\n",
    "    text = text.replace(\"?\",\"?<stop>\")\n",
    "    text = text.replace(\"!\",\"!<stop>\")\n",
    "    text = text.replace(\"<prd>\",\".\")\n",
    "    sentences = text.split(\"<stop>\")\n",
    "    sentences = sentences[:-1]\n",
    "    sentences = [s.strip() for s in sentences]\n",
    "\n",
    "    sentences_final = []\n",
    "    for sent in sentences:\n",
    "        if len(sent)>= 120:\n",
    "            if ',' in sent:\n",
    "                pos_comma = np.array([i for i in range(len(sent)) if sent.startswith(',', i)])\n",
    "                to_split = np.argmin(np.abs(pos_comma - (len(sent) - pos_comma)))\n",
    "                if (pos_comma[to_split])<= 50 or (len(sent)-pos_comma[to_split] <= 50):\n",
    "                    sentences_final.append(sent)\n",
    "                else:\n",
    "                    sentences_final.append(sent[:pos_comma[to_split]])\n",
    "                    sentences_final.append(sent[pos_comma[to_split] +1:])\n",
    "            else:\n",
    "                sentences_final.append(sent)\n",
    "        else:\n",
    "            sentences_final.append(sent)\n",
    "\n",
    "    return sentences_final\n",
    "\n",
    "\n",
    "def get_sentence_labels(url, pid, sentences, clf, model, sent_no=4):\n",
    "    prediction_probas = clf.predict_proba(model.predict_proba(sentences))\n",
    "    df = pd.DataFrame()\n",
    "    df['sentences'] = sentences\n",
    "    df['labels_1'] = np.argmax(prediction_probas,axis=1)\n",
    "    df['prob_1'] = np.max(prediction_probas,axis=1)\n",
    "    df['label_text_1'] = df['labels_1'].replace(inv_map)\n",
    "    df['labels_2'] = [[list(p).index(i) for i in sorted(p, reverse=True)][1]  for p in prediction_probas]\n",
    "    df['prob_2'] = [p[[list(p).index(i) for i in sorted(p, reverse=True)][1]]  for p in prediction_probas]\n",
    "    df['label_text_2'] = df['labels_2'].replace(inv_map)\n",
    "    df = df.sort_values('prob_1',ascending=False)\n",
    "    labels = df[['sentences','label_text_1','label_text_2']][:sent_no]\n",
    "    flag = 1  + pid*10\n",
    "    data = json.dumps(labels.set_index('sentences').to_dict('index'))\n",
    "    item_sentence = create_item(url, flag, data)\n",
    "    response = table.put_item(Item  = item_sentence)\n",
    "\n",
    "    return(labels)\n",
    "\n",
    "\n",
    "\n",
    "def get_cuts(text, sentences_all):\n",
    "    if len(text) > 3000:\n",
    "        cumsum_ = np.cumsum([len(i) for i in sentences_all])\n",
    "        chunks = np.round(len(text)/2500)\n",
    "        cutoff_ = int(len(text)/chunks)\n",
    "        cuts = [0]\n",
    "        for i in np.arange(1,chunks):\n",
    "            cutoff = cutoff_*i\n",
    "            cut = np.argmin(np.abs(cumsum_ - cutoff))\n",
    "            cuts.append(cut)\n",
    "        cuts.append(len(sentences_all))\n",
    "        \n",
    "        sentences_chunks = []\n",
    "        for c in range(0,len(cuts)-1):\n",
    "            sentences_chunks.append(sentences_all[cuts[c]:cuts[c+1]])\n",
    "        return(sentences_chunks)\n",
    "    else:\n",
    "        return([sentences_all])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.gktoday.in/topic/maharashtra-first-state-to-set-up-divyang-department/'\n",
    "text, text_act = get_data_url(url)\n",
    "if len(text) > 120:\n",
    "    sentences_all = split_into_sentences(text)\n",
    "    sentences_chunks = get_cuts(text, sentences_all)\n",
    "    pid = 0\n",
    "    for payload in sentences_chunks:\n",
    "        sentence_labels = get_sentence_labels(url,pid,payload,clf,model,sent_no=4)\n",
    "        sentence_keywords = list(set(payload) -  set(sentence_labels.sentences.values))\n",
    "        keyphrases = get_keywords_text(url,pid,sentence_keywords,extractor)\n",
    "        text = ' '.join(payload)\n",
    "        summary = get_summary(url,pid,text, summarizer, th= min(int(len(text)/10),240))\n",
    "        pid = pid+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Maharashtra: First State to Set Up Divyang department The state government of Maharashtra recently approved the formation of a separate government department for the divyang (differently abled). The announcement of this development came on the occasion of the International Day of Disabled Persons. This comes after 20 years of demand for a separate department for the welfare and security of differently abled people. Contents About Maharashtra Government’s Divyang Department - Maharashtra is the first state in India to set up a separate department for Divyang (differently-abled). - This department is established to serve the physically and mentally disabled people in the state both educationally and professionally. - It seeks to ensure the welfare of the Divyangs and effectively implement various government schemes targeting them. - As many as 2,063 posts have been created in the new department and a total outlay of Rs. 1,143 crores is provided for it. - Earlier, all complaints and issues related to differently-abled people came under the jurisdiction of Social Justice Department headed by the Minister of Social Justice and Empowerment. - All sections looking into the concerns of differently-abled people under the Social Justice and Special Assistance department will be clubbed together to form the Divyang department. - All schemes implemented for the divyangs will be brought under one roof. Earlier, they are routed through the social justice and welfare department. - Currently, the state has over 2.5 crore differently abled people. The new department will help them in the areas of education, jobs, scholarship, health, travel and rehabilitation. - It is expected to hire some 2,000 special trainers for different disabilities. About International Day of Disabled Persons The UNGA adopted a resolution establishing the International Day of Disabled Persons in 1992. This day is observed every year on December 3 to raise global awareness about issues and challenges faced by people with disabilities. It seeks to ensure the dignity, rights and general welfare of persons with disabilities. The theme for this year’s International Day of Disabilities is “Transformative solutions for inclusive development: the role of innovation in fuelling an accessible and equitable world”.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_ = 'Policy Change Request Effective 05/29/2020 Please remove the PO Box from the insured’s mailing address.  They no longer have it.  The mailing address should read: 4 South Main Street Haydenville, MA  01039 All other aspects of the policy should remain unchanged.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['PO Box', 'mailing address'], dtype='<U15')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extractor(str_, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Capital Adequacy Ratio ; economy\n",
      "Minimum Net Worth ; economy\n",
      "RBI ; economy\n",
      "Reserve Bank of India ; economy\n",
      "Risk Weighted Assets ; economy\n",
      "Tier I UCB ; economy\n",
      "UCBs ; polity\n",
      "USCs ; defence\n",
      "Urban Cooperative Banks ; economy\n",
      "bank ; economy\n",
      "banks ; economy\n",
      "cooperative banking sector ; economy\n",
      "deposit size ; economy\n",
      "deposits ; economy\n",
      "financial soundness ; economy\n",
      "minimum net worth ; economy\n",
      "phased ; science&technology\n",
      "regulation ; governance\n",
      "regulatory ; governance\n",
      "regulatory framework ; governance\n",
      "risk ; health\n",
      "unit UCBs ; defence\n"
     ]
    }
   ],
   "source": [
    "for i in keyphrases:\n",
    "    print(i,';',inv_map[clf.predict(model.predict_proba([i]))[0]] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BTM Layout Ext Devarachikkana Halli – 5 Unit Building For Sale on 20×30 A Khata – Near RTO MULTI UNIT BUILDING FOR SALE,… ₹15,000,000 MULTI UNIT BUILDING FOR SALE,… ₹15,000,000 By EBPS 4 RENTAL INCOME BUILDING FOR SALE,… ₹95,000,000 By EBPS 4 INDEPENDENT HOUSE FOR SALE',\n",
       " ' 3BHK… ₹24,000,000 By EBPS 4 INDEPENDENT HOUSE FOR SALE, 4BHK… ₹19,900,000 By EBPS 4 MULTI UNIT PROPERTY FOR SALE,… ₹18,500,000 By EBPS 4 APARTMENT FOR SALE, 2BHK Condo… ₹4,700,000 By EBPS 1 Owning a home is a keystone of wealth… both financial affluence and emotional security.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict_proba(model.predict_proba(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'summary_text': 'The 2022 Lancet Countdown on Health and Climate Change: Health at the Mercy of Fossil Fuels points out that the world’s reliance on fossil fuels increases the risk of disease, food insecurity and other illnesses related to heat. The WHO has predicted that between 2030 and 2050, climate change is expected to cause approximately 2,50,000 additional deaths per year, from malnutrition, malaria, diarrhoea and heat stress. A health-centred response to the coexisting climate, energy, and cost-of-living crises provides an opportunity to deliver a healthy, low-carbon future.'}]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarizer(text[:5000], max_length= 240, min_length=120, do_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[\"Bali summit\", \"G20\", \"G20 Summit\", \"G20 presidency\", \"German ambassador\", \"India\", \"Indian government\", \"Indonesia\", \"Russia\", \"Ukraine\", \"consensus\", \"developing world\", \"external\", \"joint communique\", \"media briefing\", \"news feed\", \"world order\"]'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "json.dumps(list(keyphrases))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "__TableName__ = 'prod1_app_data'\n",
    "client  = boto3.client('dynamodb',region_name = 'ap-south-1')\n",
    "DB  = boto3.resource('dynamodb',region_name = 'ap-south-1')\n",
    "table = DB.Table(__TableName__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.hindustantimes.com/india-news/expect-russia-to-be-part-of-all-processes-says-india-on-g20-presidency-101669906231427.html'\n",
    "auth = '1'\n",
    "flag = 1\n",
    "text = sentence_labels[0][0]\n",
    "label_1 = sentence_labels[0][1]\n",
    "label_2 = sentence_labels[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "item = create_item(url,auth, flag,text, label_1,label_2)\n",
    "response = table.put_item(Item  = item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"label_text_2\":{\"M\":{\" the Global South, such as food, fuel and fertilisers,” he said.\":{\"S\":\"society\"},\"However, Prime Minister Narendra Modi told Russian President Vladimir Putin at a meeting in September that today’s era is “not of war”.\":{\"S\":\"international relations\"},\"From time to time, both countries indicate areas of interest or priority that they may be looking at”.\":{\"S\":\"geography\"},\"India said on Thursday it expects Russia to be part of all the processes of G20 as it assumed the presidency of the grouping against the backdrop of persisting differences among its members over the Ukraine war.\":{\"S\":\"defence\"},\"Expect Russia to be part of all processes, says India on G20 presidency India, which began its year-long G20 presidency on Thursday, and Indonesia\":{\"S\":\"geography\"}}},\"label_text_1\":{\"M\":{\" the Global South, such as food, fuel and fertilisers,” he said.\":{\"S\":\"agriculture\"},\"However, Prime Minister Narendra Modi told Russian President Vladimir Putin at a meeting in September that today’s era is “not of war”.\":{\"S\":\"defence\"},\"From time to time, both countries indicate areas of interest or priority that they may be looking at”.\":{\"S\":\"international relations\"},\"India said on Thursday it expects Russia to be part of all the processes of G20 as it assumed the presidency of the grouping against the backdrop of persisting differences among its members over the Ukraine war.\":{\"S\":\"international relations\"},\"Expect Russia to be part of all processes, says India on G20 presidency India, which began its year-long G20 presidency on Thursday, and Indonesia\":{\"S\":\"international relations\"}}}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\" the Global South, such as food, fuel and fertilisers,\\\\u201d he said.\": {\"label_text_1\": \"agriculture\", \"label_text_2\": \"society\"}, \"From time to time, both countries indicate areas of interest or priority that they may be looking at\\\\u201d.\": {\"label_text_1\": \"international relations\", \"label_text_2\": \"geography\"}, \"However, Prime Minister Narendra Modi told Russian President Vladimir Putin at a meeting in September that today\\\\u2019s era is \\\\u201cnot of war\\\\u201d.\": {\"label_text_1\": \"defence\", \"label_text_2\": \"international relations\"}, \"Expect Russia to be part of all processes, says India on G20 presidency India, which began its year-long G20 presidency on Thursday, and Indonesia\": {\"label_text_1\": \"international relations\", \"label_text_2\": \"geography\"}, \"India said on Thursday it expects Russia to be part of all the processes of G20 as it assumed the presidency of the grouping against the backdrop of persisting differences among its members over the Ukraine war.\": {\"label_text_1\": \"international relations\", \"label_text_2\": \"defence\"}}'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'agriculture',\n",
       " 1: 'culture',\n",
       " 2: 'defence',\n",
       " 3: 'economy',\n",
       " 4: 'environment',\n",
       " 5: 'geography',\n",
       " 6: 'governance',\n",
       " 7: 'health',\n",
       " 8: 'history',\n",
       " 9: 'international relations',\n",
       " 10: 'polity',\n",
       " 11: 'science&technology',\n",
       " 12: 'society',\n",
       " 13: 'sports'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.04873253, 0.09007206, 0.07610773, 0.03673253, 0.04833547,\n",
       "        0.05789667, 0.2159922 , 0.05296119, 0.0213202 , 0.10592079,\n",
       "        0.04914273, 0.04692862, 0.10922971, 0.04062758]])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "model.predict_proba(['Expect Russia to be part of all processes, says India on G20 presidency India, which began its year-long G20 presidency on Thursday, and Indonesia'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.05802507, 0.24399796, 0.04785446, 0.04873429, 0.03565255,\n",
       "        0.065207  , 0.09973415, 0.07628193, 0.01154642, 0.0455373 ,\n",
       "        0.06359865, 0.06889346, 0.09427411, 0.04066266]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_proba(['The decisive moment will be September [2023] when the [G20] summit comes together.'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'agriculture': 0,\n",
       " 'culture': 1,\n",
       " 'defence': 2,\n",
       " 'economy': 3,\n",
       " 'environment': 4,\n",
       " 'geography': 5,\n",
       " 'governance': 6,\n",
       " 'health': 7,\n",
       " 'history': 8,\n",
       " 'international relations': 9,\n",
       " 'polity': 10,\n",
       " 'science&technology': 11,\n",
       " 'society': 12,\n",
       " 'sports': 13}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "289"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len('Expect Russia to be part of all processes, says India on G20 presidency India, which began its year-long G20 presidency on Thursday, and Indonesia, the previous president, played a key role in finalising a joint communique at the Bali summit amid deep divisions between Russia and the West')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len('Expect Russia to be part of all processes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3178"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([len(i) for i in sentences_final])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3186"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([len(i) for i in sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "s =  'Expect Russia to be part of all processes, says India on G20 presidency India which began its year-long G20 presidency on Thursday '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_comma = s.split(',', 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_comma = np.array([i for i in range(len(s)) if s.startswith(',', i)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmin(np.abs(pos_comma - (len(s) - pos_comma)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "146"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_comma[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Expect Russia to be part of all processes, says India on G20 presidency India, which began its year-long G20 presidency on Thursday, and Indonesia'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s[:146]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' the previous president, played a key role in finalising a joint communique at the Bali summit amid deep divisions between Russia and the West'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s[147:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([248, 212, 158, 143, 119])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " (len(s) - pos_comma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions_model(text, model, map_):\n",
    "    predictions = model.predict_proba(text.split('.'))\n",
    "    predictions_label = []\n",
    "    for i in predictions:\n",
    "        if np.max(i)> th:\n",
    "            predictions_label.append(np.argmax(predictions[0])) \n",
    "        else:\n",
    "            predictions_label.append(None) \n",
    "    return(text.split('.'), predictions_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(['environment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'agriculture': 0,\n",
       " 'culture': 1,\n",
       " 'defence': 2,\n",
       " 'economy': 3,\n",
       " 'environment': 4,\n",
       " 'geography': 5,\n",
       " 'governance': 6,\n",
       " 'health': 7,\n",
       " 'history': 8,\n",
       " 'international relations': 9,\n",
       " 'polity': 10,\n",
       " 'science&technology': 11,\n",
       " 'society': 12,\n",
       " 'sports': 13}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Expect Russia to be part of all processes, says India on G20 presidency India, which began its year-long G20 presidency on Thursday, and Indonesia, the previous president, played a key role in finalising a joint communique at the Bali summit amid deep divisions between Russia and the West',\n",
       " ' India said on Thursday it expects Russia to be part of all the processes of G20 as it assumed the presidency of the grouping against the backdrop of persisting differences among its members over the Ukraine war',\n",
       " ' “Russia is a member of the G20 and hence we would expect them to be participating in these processes,” external affairs ministry spokesperson Arindam Bagchi told a weekly media briefing while responding to questions about the divisions within the G20 over Russia’s invasion of Ukraine',\n",
       " ' Bagchi noted that the G20 works on the important principle of consensus, and India’s efforts as president of the grouping of the world’s 20 largest economies will be aimed at building consensus',\n",
       " ' This was the stance taken by India at the G20 Summit in Bali last month and this approach will continue, he said',\n",
       " ' “I would not be able to say anything further except to say that the grouping needs to speak with one voice, particularly on important issues affecting the world',\n",
       " ' We will certainly focus on issues that are affecting the developing world, the Global South, such as food, fuel and fertilisers,” he said',\n",
       " ' India, which began its year-long G20 presidency on Thursday, and Indonesia, the previous president, played a key role in finalising a joint communique at the Bali summit amid deep divisions between Russia and the West',\n",
       " ' German ambassador Philipp Ackermann said on Wednesday that coping with the fallout of Russia’s invasion of Ukraine will be one of the “most difficult issues” for India’s G20 presidency',\n",
       " ' “The decisive moment will be September [2023] when the [G20] summit comes together',\n",
       " ' But as it stands now, I think dealing with Russia will be one of the most difficult issues in this [G20] presidency,” Ackermann told reporters',\n",
       " ' Bagchi pointed out that the world order has changed and structures and institutions of the past need to change to tackle contemporary challenges',\n",
       " ' India’s part in the G20 reflects these changes and the world cannot work with the structures of the past to address the challenges of today, he said',\n",
       " ' Responding to a separate question on a media report that sanctions-hit Russia has sent India a list of more than 500 products it requires, including parts for cars, aircraft and trains and raw materials, Bagchi said: “We have regular engagement with Russia on how to sustain and expand trade',\n",
       " ' This has been going on for many years',\n",
       " ' From time to time, both countries indicate areas of interest or priority that they may be looking at',\n",
       " '” He added, “I would urge that nothing more should be read into this',\n",
       " '” The Indian government has not joined the West in openly criticising Russia’s invasion of Ukraine, and it has sharply increased purchases of Russian crude and fertilisers in recent months',\n",
       " ' However, Prime Minister Narendra Modi told Russian President Vladimir Putin at a meeting in September that today’s era is “not of war”',\n",
       " ' Personalise your news feed',\n",
       " ' Follow trending topics']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PMLA Amendments In its recent amendment, the Central Government listed 15 government bodies that are required to share information with the Enforcement Directorate under the Prevention of Money Laundering Act (PMLA)',\n",
       " ' The list was amended in accordance with Section 66 of the PMLA',\n",
       " ' Contents Which are the agencies that were recently included in the list? The government entities that were recently included to share information with the Enforcement Directorate are: - National Investigating Agency (NIA) - Serious Fraud Investigation Office (SFIO) - State Police - Director General of Foreign Trade (DGFT) - Ministry of External Affairs (MEA) - National Intelligence Grid - Central Vigilance Commission (CVC) - Defence Intelligence Agency - National Technical Research Organisation (NTRO) - Military Intelligence - Wildlife Crime Control Bureau - Competition Commission of India (CCI) Under Section 66 of the Act, 15 entities are bound to disclose and share case information to the ED if the investigation falls under its jurisdiction',\n",
       " ' The information shared can be used by the Enforcement Directorate to file a case under the PMLA when it deems fit',\n",
       " ' Before this notification, only the Director (Financial Intelligence Unit, India, under the Union Finance Ministry;s Revenue Department), Cabinet Secretariat (RAW), National Security Council Secretariat, Intelligence Bureau, Economic Offences Wing of Central Bureau of Investigation (CBI), Chief Secretaries of State governments, RBI, Department of Company Affairs, and SEBI were required to share information with the Enforcement Directorate',\n",
       " ' The Special Investigation Team (SIT) set up by the Finance Ministry and inquiry appointed under the Civil Services Rules or Public Services Inquiry Act or any other preliminary enquiry appointed with the approval of Central Vigilance Commission are also included in this list',\n",
       " ' What is Enforcement Directorate? The Enforcement Directorate (ED), which comes under the aegis of the Finance Ministry’s Revenue Department, is an economic intelligence agency responsible for combating economic crimes in India and enforcing economic laws',\n",
       " ' Its origins can be traced back to the Enforcement Unit, which was set up under the Foreign Exchange Regulation Act, 1947 to combat violations of Exchange Control Laws',\n",
       " ' The Enforcement Unit was renamed as the Enforcement Directorate in 1957',\n",
       " ' Month: Current Affairs - December, 2022 Category: India Nation & States Current Affairs Topics: economic intelligence unit • Enforcement Directorate • PMLA • PMLA Act • Prevention of Money Laundering Act (PMLA) Latest E-Books Current Affairs [PDF] - November 16-30, 2022 |₹100',\n",
       " '00| Current Affairs [PDF] - November 1-15, 2022 |₹100',\n",
       " '00| Current Affairs [PDF] - October 16-31, 2022 |₹100',\n",
       " '00| |View All E-Books: Recent Release| Comments']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "keywords = []\n",
    "for sent in text.split('.'):\n",
    "    for i in nlp(sent):\n",
    "        keywords.append(i['word'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Besides', 'Covid', 'Disaster risk management', 'G20', 'G20 forum',\n",
       "       'Global South', 'India', 'India’', 'Modi administration',\n",
       "       'NEW DELHI', 'Sherpa Amitabh Kant', 'Sources', 'TOI', 'Thursday'],\n",
       "      dtype='<U24')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NEW DELHI: The government is seeking to bring climate finance, disaster risk reduction and startups to the forefront of the G20 agenda as India begins its presidency of the group of the world’s richest countries on Thursday, aiming to showcase its achievements and offer solutions around digital payments and e-services to position them as digital public goods. Disaster risk management and startups have made their way into the agenda for the first time, according to the details accessed by TOI. The government is keen to ensure global dialogue on building disaster and climate-resilient infrastructure. It also wants to ensure global dialogue on an early warning mechanism for certain types of disasters that affect lives in coastal areas. Given the need to push the climate agenda, the Modi administration will also seek that issues around financing, an areas where the developed countries have not kept their word, and a thrust to circular economy are addressed. Given the geo-political tensions at a time when the global economy is just recovering from the pandemic, India will also try to get countries that account for nearly 85% of global GDP to address issues around spillover effects of rate hikes in advanced economies on the emerging markets. Besides, the finance ministry is keen to get G20 members to work on creating a set of rules to deal with crypto currency. Sources said India will also seek to build consensus on issues around debt sustainability and reform of the multilateral systems to reflect the new realities. “India’s G20 presidency comes at an opportune time, not just for India but also the world. With the world already amidst a climate crisis, the Covid crisis, subsequent disruptions in global supply chains and debt crises, geopolitical crisis, and ensuing food and energy crises have thrown economies in turmoil, especially developing ones. \"The G20 forum is an effective platform to drive change at a global level. This will be crucial for the world to navigate its way through the manifold crises of today. It is a time for collective action, and there is no country placed better than India to drive consensus and action. This is an opportunity for India to position itself as the voice of the Global South, acting as a bridge between developing and developed nations. \"The G20 presidency will reaffirm global trust in India as a responsible power, with inclusivity and action at the heart of its G20 agenda, making it a catalyst for global change,” India’s Sherpa Amitabh Kant told TOI.'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "class TextRank4Keyword():\n",
    "    \"\"\"Extract keywords from text\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.d = 0.85 # damping coefficient, usually is .85\n",
    "        self.min_diff = 1e-5 # convergence threshold\n",
    "        self.steps = 10 # iteration steps\n",
    "        self.node_weight = None # save keywords and its weight\n",
    "\n",
    "    \n",
    "    def set_stopwords(self, stopwords):  \n",
    "        \"\"\"Set stop words\"\"\"\n",
    "        for word in STOP_WORDS.union(set(stopwords)):\n",
    "            lexeme = nlp.vocab[word]\n",
    "            lexeme.is_stop = True\n",
    "    \n",
    "    def sentence_segment(self, doc, candidate_pos, lower):\n",
    "        \"\"\"Store those words only in cadidate_pos\"\"\"\n",
    "        sentences = []\n",
    "        for sent in doc.sents:\n",
    "            selected_words = []\n",
    "            for token in sent:\n",
    "                # Store words only with cadidate POS tag\n",
    "                if token.pos_ in candidate_pos and token.is_stop is False:\n",
    "                    if lower is True:\n",
    "                        selected_words.append(token.text.lower())\n",
    "                    else:\n",
    "                        selected_words.append(token.text)\n",
    "            sentences.append(selected_words)\n",
    "        return sentences\n",
    "        \n",
    "    def get_vocab(self, sentences):\n",
    "        \"\"\"Get all tokens\"\"\"\n",
    "        vocab = OrderedDict()\n",
    "        i = 0\n",
    "        for sentence in sentences:\n",
    "            for word in sentence:\n",
    "                if word not in vocab:\n",
    "                    vocab[word] = i\n",
    "                    i += 1\n",
    "        return vocab\n",
    "    \n",
    "    def get_token_pairs(self, window_size, sentences):\n",
    "        \"\"\"Build token_pairs from windows in sentences\"\"\"\n",
    "        token_pairs = list()\n",
    "        for sentence in sentences:\n",
    "            for i, word in enumerate(sentence):\n",
    "                for j in range(i+1, i+window_size):\n",
    "                    if j >= len(sentence):\n",
    "                        break\n",
    "                    pair = (word, sentence[j])\n",
    "                    if pair not in token_pairs:\n",
    "                        token_pairs.append(pair)\n",
    "        return token_pairs\n",
    "        \n",
    "    def symmetrize(self, a):\n",
    "        return a + a.T - np.diag(a.diagonal())\n",
    "    \n",
    "    def get_matrix(self, vocab, token_pairs):\n",
    "        \"\"\"Get normalized matrix\"\"\"\n",
    "        # Build matrix\n",
    "        vocab_size = len(vocab)\n",
    "        g = np.zeros((vocab_size, vocab_size), dtype='float')\n",
    "        for word1, word2 in token_pairs:\n",
    "            i, j = vocab[word1], vocab[word2]\n",
    "            g[i][j] = 1\n",
    "            \n",
    "        # Get Symmeric matrix\n",
    "        g = self.symmetrize(g)\n",
    "        \n",
    "        # Normalize matrix by column\n",
    "        norm = np.sum(g, axis=0)\n",
    "        g_norm = np.divide(g, norm, where=norm!=0) # this is ignore the 0 element in norm\n",
    "        \n",
    "        return g_norm\n",
    "\n",
    "    \n",
    "    def get_keywords(self, number=10):\n",
    "        \"\"\"Print top number keywords\"\"\"\n",
    "        node_weight = OrderedDict(sorted(self.node_weight.items(), key=lambda t: t[1], reverse=True))\n",
    "        for i, (key, value) in enumerate(node_weight.items()):\n",
    "            print(key + ' - ' + str(value))\n",
    "            if i > number:\n",
    "                break\n",
    "        \n",
    "        \n",
    "    def analyze(self, text, \n",
    "                candidate_pos=['NOUN', 'PROPN'], \n",
    "                window_size=4, lower=False, stopwords=list()):\n",
    "        \"\"\"Main function to analyze text\"\"\"\n",
    "        \n",
    "        # Set stop words\n",
    "        self.set_stopwords(stopwords)\n",
    "        \n",
    "        # Pare text by spaCy\n",
    "        doc = nlp(text)\n",
    "        \n",
    "        # Filter sentences\n",
    "        sentences = self.sentence_segment(doc, candidate_pos, lower) # list of list of words\n",
    "        \n",
    "        # Build vocabulary\n",
    "        vocab = self.get_vocab(sentences)\n",
    "        \n",
    "        # Get token_pairs from windows\n",
    "        token_pairs = self.get_token_pairs(window_size, sentences)\n",
    "        \n",
    "        # Get normalized matrix\n",
    "        g = self.get_matrix(vocab, token_pairs)\n",
    "        \n",
    "        # Initionlization for weight(pagerank value)\n",
    "        pr = np.array([1] * len(vocab))\n",
    "        \n",
    "        # Iteration\n",
    "        previous_pr = 0\n",
    "        for epoch in range(self.steps):\n",
    "            pr = (1-self.d) + self.d * np.dot(g, pr)\n",
    "            if abs(previous_pr - sum(pr))  < self.min_diff:\n",
    "                break\n",
    "            else:\n",
    "                previous_pr = sum(pr)\n",
    "\n",
    "        # Get weight for each node\n",
    "        node_weight = dict()\n",
    "        for word, index in vocab.items():\n",
    "            node_weight[word] = pr[index]\n",
    "        \n",
    "        self.node_weight = node_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "India - 4.475863058211749\n",
      "G20 - 3.210650043769627\n",
      "agenda - 2.7650726053998467\n",
      "countries - 2.482918651168669\n",
      "issues - 2.4047668706634053\n",
      "climate - 2.0973085173341097\n",
      "time - 1.8610261827381995\n",
      "world - 1.7904226189397052\n",
      "areas - 1.674571110650254\n",
      "debt - 1.6570284554390828\n",
      "crises - 1.596109029543607\n",
      "seek - 1.5788841729121734\n"
     ]
    }
   ],
   "source": [
    "tr4w = TextRank4Keyword()\n",
    "tr4w.analyze(text, candidate_pos = ['NOUN','PROPN', 'VERB'], window_size=10, lower=False)\n",
    "tr4w.get_keywords(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NEW DELHI: The government is seeking to bring climate finance, disaster risk reduction and startups to the forefront of the G20 agenda as India begins its presidency of the group of the world’s richest countries on Thursday, aiming to showcase its achievements and offer solutions around digital payments and e-services to position them as digital public goods. Disaster risk management and startups have made their way into the agenda for the first time, according to the details accessed by TOI. The government is keen to ensure global dialogue on building disaster and climate-resilient infrastructure. It also wants to ensure global dialogue on an early warning mechanism for certain types of disasters that affect lives in coastal areas. Given the need to push the climate agenda, the Modi administration will also seek that issues around financing, an areas where the developed countries have not kept their word, and a thrust to circular economy are addressed. Given the geo-political tensions at a time when the global economy is just recovering from the pandemic, India will also try to get countries that account for nearly 85% of global GDP to address issues around spillover effects of rate hikes in advanced economies on the emerging markets. Besides, the finance ministry is keen to get G20 members to work on creating a set of rules to deal with crypto currency. Sources said India will also seek to build consensus on issues around debt sustainability and reform of the multilateral systems to reflect the new realities. “India’s G20 presidency comes at an opportune time, not just for India but also the world. With the world already amidst a climate crisis, the Covid crisis, subsequent disruptions in global supply chains and debt crises, geopolitical crisis, and ensuing food and energy crises have thrown economies in turmoil, especially developing ones. \"The G20 forum is an effective platform to drive change at a global level. This will be crucial for the world to navigate its way through the manifold crises of today. It is a time for collective action, and there is no country placed better than India to drive consensus and action. This is an opportunity for India to position itself as the voice of the Global South, acting as a bridge between developing and developed nations. \"The G20 presidency will reaffirm global trust in India as a responsible power, with inclusivity and action at the heart of its G20 agenda, making it a catalyst for global change,” India’s Sherpa Amitabh Kant told TOI.'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "get_label() missing 1 required positional argument: 'model_sim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m nlp(df\u001b[38;5;241m.\u001b[39mheadings\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;241m14\u001b[39m]):\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(i[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mword\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[43mget_label\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mword\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mTypeError\u001b[0m: get_label() missing 1 required positional argument: 'model_sim'"
     ]
    }
   ],
   "source": [
    "for i in nlp(df.headings.values[14]):\n",
    "    print(i['word'], get_label(i['word'],model_sim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'agriculture'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_label('apple',model_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bangladesh allows commercial cultivation of Bt Brinjal'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.headings.values[14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('data/training_data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Collecting trafilatura\n",
      "  Downloading trafilatura-1.4.0-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting justext>=3.0.0\n",
      "  Downloading jusText-3.0.0-py2.py3-none-any.whl (837 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m837.8/837.8 kB\u001b[0m \u001b[31m44.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting htmldate>=1.3.2\n",
      "  Downloading htmldate-1.4.0-py3-none-any.whl (33 kB)\n",
      "Requirement already satisfied: charset-normalizer>=2.1.1 in ./deployenv/lib/python3.10/site-packages (from trafilatura) (2.1.1)\n",
      "Collecting lxml>=4.6.4\n",
      "  Downloading lxml-4.9.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (6.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m51.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: certifi in ./deployenv/lib/python3.10/site-packages (from trafilatura) (2022.9.24)\n",
      "Collecting urllib3<2,>=1.26\n",
      "  Downloading urllib3-1.26.13-py2.py3-none-any.whl (140 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.6/140.6 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting courlan>=0.8.3\n",
      "  Downloading courlan-0.8.3-py3-none-any.whl (34 kB)\n",
      "Collecting tld>=0.12.6\n",
      "  Downloading tld-0.12.6-py39-none-any.whl (412 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m412.2/412.2 kB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting langcodes>=3.3.0\n",
      "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.6/181.6 kB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting charset-normalizer>=2.1.1\n",
      "  Downloading charset_normalizer-3.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (198 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m198.8/198.8 kB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting dateparser>=1.1.2\n",
      "  Downloading dateparser-1.1.4-py2.py3-none-any.whl (292 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m292.9/292.9 kB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in ./deployenv/lib/python3.10/site-packages (from htmldate>=1.3.2->trafilatura) (2.8.2)\n",
      "Requirement already satisfied: pytz in ./deployenv/lib/python3.10/site-packages (from dateparser>=1.1.2->htmldate>=1.3.2->trafilatura) (2022.6)\n",
      "Requirement already satisfied: tzlocal in ./deployenv/lib/python3.10/site-packages (from dateparser>=1.1.2->htmldate>=1.3.2->trafilatura) (4.2)\n",
      "Requirement already satisfied: regex!=2019.02.19,!=2021.8.27 in ./deployenv/lib/python3.10/site-packages (from dateparser>=1.1.2->htmldate>=1.3.2->trafilatura) (2022.10.31)\n",
      "Requirement already satisfied: six>=1.5 in ./deployenv/lib/python3.10/site-packages (from python-dateutil>=2.8.2->htmldate>=1.3.2->trafilatura) (1.15.0)\n",
      "Requirement already satisfied: pytz-deprecation-shim in ./deployenv/lib/python3.10/site-packages (from tzlocal->dateparser>=1.1.2->htmldate>=1.3.2->trafilatura) (0.1.0.post0)\n",
      "Requirement already satisfied: tzdata in ./deployenv/lib/python3.10/site-packages (from pytz-deprecation-shim->tzlocal->dateparser>=1.1.2->htmldate>=1.3.2->trafilatura) (2022.6)\n",
      "Installing collected packages: charset-normalizer, urllib3, tld, lxml, langcodes, justext, courlan, dateparser, htmldate, trafilatura\n",
      "  Attempting uninstall: charset-normalizer\n",
      "    Found existing installation: charset-normalizer 2.1.1\n",
      "    Uninstalling charset-normalizer-2.1.1:\n",
      "      Successfully uninstalled charset-normalizer-2.1.1\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.25.11\n",
      "    Uninstalling urllib3-1.25.11:\n",
      "      Successfully uninstalled urllib3-1.25.11\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "requests 2.28.1 requires charset-normalizer<3,>=2, but you have charset-normalizer 3.0.1 which is incompatible.\n",
      "aiohttp 3.8.3 requires charset-normalizer<3.0,>=2.0, but you have charset-normalizer 3.0.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed charset-normalizer-3.0.1 courlan-0.8.3 dateparser-1.1.4 htmldate-1.4.0 justext-3.0.0 langcodes-3.3.0 lxml-4.9.1 tld-0.12.6 trafilatura-1.4.0 urllib3-1.26.13\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install trafilatura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I\\'m writing some code (Python) to scrape text from web pages. My goal is to find a way to filter/delete the paragraphs on webpages that are not in the main article (e.g. advertisement, links to other articles, etc.). So far I\\'ve been using the .find_all(\"p\") command to extract only paragraphs from the text, which although successful also scrapes a lot of rudimentary paragraphs which are not in the main/body of each article. This is my code now: from urllib.request import Request, urlopen from bs4 import BeautifulSoup URLs = [ \"https://www.elsoldetoluca.com.mx/local/proponen-sistemas-para-captar-agua-pluvial-en-el-edomex-6585661.html\", \"https://www.elsoldetoluca.com.mx/local/agua-de-acuifero-del-valle-de-toluca-solo-debe-ser-para-uso-de-consumo-humano-especialista-4146232.html\" ] for url in URLs: req = Request(url, headers={\"User-Agent\": \\'Mozilla/5.0\\'}) page = urlopen(req) paragraphs = [] htmlParse = BeautifulSoup(page.read(), \\'lxml\\') for para in htmlParse.find_all(\"p\"): paragraph = para.get_text().replace(\"\\\\n\", \" \") paragraphs = paragraphs + [paragraph] text = str(\"\\\\n\\\\n\".join(paragraphs)) So I\\'m looking for a clever way to filter out the paragraphs that are not in the main article. It is essential that this method can be applied on any webpage, since I\\'m using this code on ~100 random websites. Things I\\'ve been looking into already is to filter paragraphs containing certain words, however, I would rather not do this because this way a lot of information/paragraphs are left out. Also, I\\'ve been looking at leaving out HTML sections with certain names (such as https://matix.io/extract-text-from-webpage-using-beautifulsoup-and-python/), but I find this is not very effective... Anyone any tips on how to do this elegantly? Thanks! <article>element or a <div class=\"article\">… then you can search for the paragraphs from this root element using the way you do.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_extracted.replace('\\n',' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENTRYPOINT [\"uvicorn main:app --host\", \"0.0.0.0\", \"--port\", \"80\"]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('deployenv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9d43577eea919236b69ebd0f8998005d41bfb2309937fa01b8dc5c2f454a02cb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
