{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "/home/ubuntu/upsc_models_deployment/deployenv/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/ubuntu/upsc_models_deployment/deployenv/lib/python3.10/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator LogisticRegression from version 1.0.1 when using version 1.1.3. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk import sent_tokenize\n",
    "nltk.download('punkt')\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from setfit import SetFitModel, SetFitTrainer\n",
    "from datasets import load_dataset\n",
    "from sentence_transformers.losses import CosineSimilarityLoss\n",
    "import datasets\n",
    "from datasets import Dataset, DatasetDict\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import trafilatura\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import (\n",
    "    TokenClassificationPipeline,\n",
    "    AutoModelForTokenClassification,\n",
    "    AutoTokenizer)\n",
    "from transformers.pipelines import AggregationStrategy\n",
    "import numpy as np\n",
    "import pickle\n",
    "import re\n",
    "import boto3\n",
    "import json\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Jean-Baptiste/camembert-ner\")\n",
    "model_ner = AutoModelForTokenClassification.from_pretrained(\"Jean-Baptiste/camembert-ner\")\n",
    "model_sim = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "nlp = pipeline('ner', model=model_ner, tokenizer=tokenizer, aggregation_strategy=\"simple\")\n",
    "model = SetFitModel.from_pretrained(\"kowshik/upsc-classification-model-v1\")\n",
    "model_name = \"ml6team/keyphrase-extraction-kbir-inspec\"\n",
    "map_ = {'agriculture': 0,'culture': 1,'defence': 2,'economy': 3,'environment': 4,'geography': 5,'governance': 6,\n",
    "'health': 7,'history': 8,'international relations': 9,'polity': 10,'science&technology': 11,'society': 12,'sports': 13}\n",
    "inv_map = {v: k for k, v in map_.items()}\n",
    "\n",
    "__TableName__ = 'prod1_app_data'\n",
    "client  = boto3.client('dynamodb',region_name = 'ap-south-1')\n",
    "DB  = boto3.resource('dynamodb',region_name = 'ap-south-1')\n",
    "table = DB.Table(__TableName__)\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "def get_summary(url,pid,text,summarizer,th=120):\n",
    "    if len(text) > 1200:\n",
    "        summary = summarizer(text, max_length= th, min_length=120, do_sample=False)[0]['summary_text']\n",
    "        flag = 3 + pid*10\n",
    "        data = summary\n",
    "        item_summary = create_item(url,flag, data)\n",
    "        response = table.put_item(Item  = item_summary)\n",
    "        return(summary)\n",
    "    else:\n",
    "        flag = 3 + pid*10\n",
    "        data = text\n",
    "        item_summary = create_item(url,flag, data)\n",
    "        response = table.put_item(Item  = item_summary)\n",
    "        return(text)\n",
    "\n",
    "f = open('rm_model.pkl', 'rb')\n",
    "clf = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "def create_item(url,flag, data):\n",
    "    '''  \n",
    "    flag = 0 > text\n",
    "    flag = 1 > sentence\n",
    "    flag = 2 > key Phrase \n",
    "    flag = 3 > summary \n",
    "    '''\n",
    "    item = {\n",
    "        'url': url,\n",
    "        'flag':flag,\n",
    "        'data': data,\n",
    "    }\n",
    "    return(item)\n",
    "\n",
    "\n",
    "def get_data_url(url):\n",
    "    downloaded = trafilatura.fetch_url(url)\n",
    "    text_original = trafilatura.extract(downloaded)\n",
    "    text_extracted = text_original.replace('\\n',' ')\n",
    "    flag = 0\n",
    "    data = text_original\n",
    "    item_complete = create_item(url,flag, data)\n",
    "    response = table.put_item(Item  = item_complete)\n",
    "    return(text_extracted, text_original)\n",
    "\n",
    "def get_label(word,model_sim):\n",
    "    labels = ['Environment','Geography','International Relations',\n",
    "    'Polity','Governance','Health','Society','Economy','Science&Technology','Agriculture','sports']\n",
    "    labels = [i.lower() for i in labels]\n",
    "    embeddings_tags = model_sim.encode(labels)\n",
    "    embeddings_key = model_sim.encode(word)\n",
    "    probs = cosine_similarity([embeddings_key],embeddings_tags)\n",
    "    label_index = np.argmax(probs)\n",
    "    return(labels[label_index])\n",
    "\n",
    "class KeyphraseExtractionPipeline(TokenClassificationPipeline):\n",
    "    def __init__(self, model, *args, **kwargs):\n",
    "        super().__init__(\n",
    "            model=AutoModelForTokenClassification.from_pretrained(model),\n",
    "            tokenizer=AutoTokenizer.from_pretrained(model),\n",
    "            *args,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "    def postprocess(self, model_outputs):\n",
    "        results = super().postprocess(\n",
    "            model_outputs=model_outputs,\n",
    "            aggregation_strategy=AggregationStrategy.SIMPLE,\n",
    "        )\n",
    "        return np.unique([result.get(\"word\").strip() for result in results])\n",
    "extractor = KeyphraseExtractionPipeline(model=model_name)\n",
    "\n",
    "def get_keywords_text(url,pid, sentences, extractor):\n",
    "    keywords_ = []\n",
    "    for te in sentences:\n",
    "        keywords_ = keywords_+ list(extractor(te))\n",
    "    keywords_unq = np.unique(keywords_)\n",
    "    flag = 2 +pid*10\n",
    "    data = json.dumps(list(keywords_unq))\n",
    "    item_key = create_item(url,  flag, data,)\n",
    "    response = table.put_item(Item  = item_key)\n",
    "    return(keywords_unq)\n",
    "\n",
    "alphabets= \"([A-Za-z])\"\n",
    "prefixes = \"(Mr|St|Mrs|Ms|Dr)[.]\"\n",
    "suffixes = \"(Inc|Ltd|Jr|Sr|Co)\"\n",
    "starters = \"(Mr|Mrs|Ms|Dr|He\\s|She\\s|It\\s|They\\s|Their\\s|Our\\s|We\\s|But\\s|However\\s|That\\s|This\\s|Wherever)\"\n",
    "acronyms = \"([A-Z][.][A-Z][.](?:[A-Z][.])?)\"\n",
    "websites = \"[.](com|net|org|io|gov)\"\n",
    "digits = \"([0-9])\"\n",
    "\n",
    "def split_into_sentences(text):\n",
    "    text = \" \" + text + \"  \"\n",
    "    text = text.replace(\"\\n\",\" \")\n",
    "    text = re.sub(prefixes,\"\\\\1<prd>\",text)\n",
    "    text = re.sub(websites,\"<prd>\\\\1\",text)\n",
    "    text = re.sub(digits + \"[.]\" + digits,\"\\\\1<prd>\\\\2\",text)\n",
    "    if \"...\" in text: text = text.replace(\"...\",\"<prd><prd><prd>\")\n",
    "    if \"Ph.D\" in text: text = text.replace(\"Ph.D.\",\"Ph<prd>D<prd>\")\n",
    "    text = re.sub(\"\\s\" + alphabets + \"[.] \",\" \\\\1<prd> \",text)\n",
    "    text = re.sub(acronyms+\" \"+starters,\"\\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\\\\3<prd>\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.] \"+starters,\" \\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.]\",\" \\\\1<prd>\",text)\n",
    "    text = re.sub(\" \" + alphabets + \"[.]\",\" \\\\1<prd>\",text)\n",
    "    if \"”\" in text: text = text.replace(\".”\",\"”.\")\n",
    "    if \"\\\"\" in text: text = text.replace(\".\\\"\",\"\\\".\")\n",
    "    if \"!\" in text: text = text.replace(\"!\\\"\",\"\\\"!\")\n",
    "    if \"?\" in text: text = text.replace(\"?\\\"\",\"\\\"?\")\n",
    "    text = text.replace(\".\",\".<stop>\")\n",
    "    text = text.replace(\"?\",\"?<stop>\")\n",
    "    text = text.replace(\"!\",\"!<stop>\")\n",
    "    text = text.replace(\"<prd>\",\".\")\n",
    "    sentences = text.split(\"<stop>\")\n",
    "    sentences = sentences[:-1]\n",
    "    sentences = [s.strip() for s in sentences]\n",
    "\n",
    "    sentences_final = []\n",
    "    for sent in sentences:\n",
    "        if len(sent)>= 120:\n",
    "            if ',' in sent:\n",
    "                pos_comma = np.array([i for i in range(len(sent)) if sent.startswith(',', i)])\n",
    "                to_split = np.argmin(np.abs(pos_comma - (len(sent) - pos_comma)))\n",
    "                if (pos_comma[to_split])<= 50 or (len(sent)-pos_comma[to_split] <= 50):\n",
    "                    sentences_final.append(sent)\n",
    "                else:\n",
    "                    sentences_final.append(sent[:pos_comma[to_split]])\n",
    "                    sentences_final.append(sent[pos_comma[to_split] +1:])\n",
    "            else:\n",
    "                sentences_final.append(sent)\n",
    "        else:\n",
    "            sentences_final.append(sent)\n",
    "\n",
    "    return sentences_final\n",
    "\n",
    "\n",
    "def get_sentence_labels(url, pid, sentences, clf, model, sent_no=4):\n",
    "    prediction_probas = clf.predict_proba(model.predict_proba(sentences))\n",
    "    df = pd.DataFrame()\n",
    "    df['sentences'] = sentences\n",
    "    df['labels_1'] = np.argmax(prediction_probas,axis=1)\n",
    "    df['prob_1'] = np.max(prediction_probas,axis=1)\n",
    "    df['label_text_1'] = df['labels_1'].replace(inv_map)\n",
    "    df['labels_2'] = [[list(p).index(i) for i in sorted(p, reverse=True)][1]  for p in prediction_probas]\n",
    "    df['prob_2'] = [p[[list(p).index(i) for i in sorted(p, reverse=True)][1]]  for p in prediction_probas]\n",
    "    df['label_text_2'] = df['labels_2'].replace(inv_map)\n",
    "    df = df.sort_values('prob_1',ascending=False)\n",
    "    labels = df[['sentences','label_text_1','label_text_2']][:sent_no]\n",
    "    flag = 1  + pid*10\n",
    "    data = json.dumps(labels.set_index('sentences').to_dict('index'))\n",
    "    item_sentence = create_item(url, flag, data)\n",
    "    response = table.put_item(Item  = item_sentence)\n",
    "\n",
    "    return(labels)\n",
    "\n",
    "\n",
    "\n",
    "def get_cuts(text, sentences_all):\n",
    "    if len(text) > 3000:\n",
    "        cumsum_ = np.cumsum([len(i) for i in sentences_all])\n",
    "        chunks = np.round(len(text)/2500)\n",
    "        cutoff_ = int(len(text)/chunks)\n",
    "        cuts = [0]\n",
    "        for i in np.arange(1,chunks):\n",
    "            cutoff = cutoff_*i\n",
    "            cut = np.argmin(np.abs(cumsum_ - cutoff))\n",
    "            cuts.append(cut)\n",
    "        cuts.append(len(sentences_all))\n",
    "        \n",
    "        sentences_chunks = []\n",
    "        for c in range(0,len(cuts)-1):\n",
    "            sentences_chunks.append(sentences_all[cuts[c]:cuts[c+1]])\n",
    "        return(sentences_chunks)\n",
    "    else:\n",
    "        return([sentences_all])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''  \n",
    "import boto3\n",
    "df = pd.read_pickle('/home/ubuntu/scrape_upsc_db/data/article_links.pkl')\n",
    "links = df[:100]\n",
    "items_list = []\n",
    "for i in links:\n",
    "    items = {\n",
    "            'user': 'kowshikchilamkurthy@gmail.com',\n",
    "            'url':i,\n",
    "        }\n",
    "    items_list.append(items)\n",
    "\n",
    "__TableName__ = 'prod1_user_data'\n",
    "client_user  = boto3.client('dynamodb',region_name = 'ap-south-1')\n",
    "DB_user  = boto3.resource('dynamodb',region_name = 'ap-south-1')\n",
    "table_user = DB_user.Table(__TableName__)\n",
    "for item_ in items_list:\n",
    "    response = table_user.put_item(Item  = item_)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "('incompatible input type', <class 'NoneType'>)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://www.thehindu.com/news/national/all-party-meet-ahead-of-budget-session-opposition-raises-adani-issue-ysr-congress-calls-for-caste-based-economic-census/article66450221.ec\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 2\u001b[0m text, text_act \u001b[38;5;241m=\u001b[39m \u001b[43mget_data_url\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(text) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m120\u001b[39m:\n\u001b[1;32m      4\u001b[0m     sentences_all \u001b[38;5;241m=\u001b[39m split_into_sentences(text)\n",
      "Cell \u001b[0;32mIn [2], line 83\u001b[0m, in \u001b[0;36mget_data_url\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_data_url\u001b[39m(url):\n\u001b[1;32m     82\u001b[0m     downloaded \u001b[38;5;241m=\u001b[39m trafilatura\u001b[38;5;241m.\u001b[39mfetch_url(url)\n\u001b[0;32m---> 83\u001b[0m     text_original \u001b[38;5;241m=\u001b[39m \u001b[43mtrafilatura\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdownloaded\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m     text_extracted \u001b[38;5;241m=\u001b[39m text_original\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     85\u001b[0m     flag \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/upsc_models_deployment/deployenv/lib/python3.10/site-packages/trafilatura/core.py:1050\u001b[0m, in \u001b[0;36mextract\u001b[0;34m(filecontent, url, record_id, no_fallback, favor_precision, favor_recall, include_comments, output_format, tei_validation, target_language, include_tables, include_images, include_formatting, include_links, deduplicate, date_extraction_params, only_with_metadata, with_metadata, max_tree_size, url_blacklist, author_blacklist, settingsfile, config, **kwargs)\u001b[0m\n\u001b[1;32m   1048\u001b[0m \u001b[39m# extraction\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1050\u001b[0m     document \u001b[39m=\u001b[39m bare_extraction(\n\u001b[1;32m   1051\u001b[0m         filecontent, url\u001b[39m=\u001b[39;49murl, no_fallback\u001b[39m=\u001b[39;49mno_fallback,\n\u001b[1;32m   1052\u001b[0m         favor_precision\u001b[39m=\u001b[39;49mfavor_precision, favor_recall\u001b[39m=\u001b[39;49mfavor_recall,\n\u001b[1;32m   1053\u001b[0m         include_comments\u001b[39m=\u001b[39;49minclude_comments, output_format\u001b[39m=\u001b[39;49moutput_format,\n\u001b[1;32m   1054\u001b[0m         target_language\u001b[39m=\u001b[39;49mtarget_language, include_tables\u001b[39m=\u001b[39;49minclude_tables,\n\u001b[1;32m   1055\u001b[0m         include_images\u001b[39m=\u001b[39;49minclude_images,\n\u001b[1;32m   1056\u001b[0m         include_formatting\u001b[39m=\u001b[39;49minclude_formatting, include_links\u001b[39m=\u001b[39;49minclude_links,\n\u001b[1;32m   1057\u001b[0m         deduplicate\u001b[39m=\u001b[39;49mdeduplicate,\n\u001b[1;32m   1058\u001b[0m         date_extraction_params\u001b[39m=\u001b[39;49mdate_extraction_params,\n\u001b[1;32m   1059\u001b[0m         only_with_metadata\u001b[39m=\u001b[39;49monly_with_metadata, with_metadata\u001b[39m=\u001b[39;49mwith_metadata,\n\u001b[1;32m   1060\u001b[0m         max_tree_size\u001b[39m=\u001b[39;49mmax_tree_size, url_blacklist\u001b[39m=\u001b[39;49murl_blacklist,\n\u001b[1;32m   1061\u001b[0m         author_blacklist\u001b[39m=\u001b[39;49mauthor_blacklist,\n\u001b[1;32m   1062\u001b[0m         as_dict\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, config\u001b[39m=\u001b[39;49mconfig,\n\u001b[1;32m   1063\u001b[0m     )\n\u001b[1;32m   1064\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m:\n\u001b[1;32m   1065\u001b[0m     LOGGER\u001b[39m.\u001b[39merror(\u001b[39m'\u001b[39m\u001b[39mProcessing timeout for \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m, url)\n",
      "File \u001b[0;32m~/upsc_models_deployment/deployenv/lib/python3.10/site-packages/trafilatura/core.py:865\u001b[0m, in \u001b[0;36mbare_extraction\u001b[0;34m(filecontent, url, no_fallback, favor_precision, favor_recall, include_comments, output_format, target_language, include_tables, include_images, include_formatting, include_links, deduplicate, date_extraction_params, only_with_metadata, with_metadata, max_tree_size, url_blacklist, author_blacklist, as_dict, config)\u001b[0m\n\u001b[1;32m    856\u001b[0m \u001b[39m#if no_fallback is True:\u001b[39;00m\n\u001b[1;32m    857\u001b[0m \u001b[39m#    fast = no_fallback\u001b[39;00m\n\u001b[1;32m    858\u001b[0m     \u001b[39m#warnings.warn(\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    862\u001b[0m \n\u001b[1;32m    863\u001b[0m \u001b[39m# load data\u001b[39;00m\n\u001b[1;32m    864\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 865\u001b[0m     tree \u001b[39m=\u001b[39m load_html(filecontent)\n\u001b[1;32m    866\u001b[0m     \u001b[39mif\u001b[39;00m tree \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    867\u001b[0m         LOGGER\u001b[39m.\u001b[39merror(\u001b[39m'\u001b[39m\u001b[39mempty HTML tree for URL \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m, url)\n",
      "File \u001b[0;32m~/upsc_models_deployment/deployenv/lib/python3.10/site-packages/trafilatura/utils.py:171\u001b[0m, in \u001b[0;36mload_html\u001b[0;34m(htmlobject)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[39m# do not accept any other type after this point\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(htmlobject, (\u001b[39mbytes\u001b[39m, \u001b[39mstr\u001b[39m)):\n\u001b[0;32m--> 171\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mincompatible input type\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mtype\u001b[39m(htmlobject))\n\u001b[1;32m    172\u001b[0m \u001b[39m# start processing\u001b[39;00m\n\u001b[1;32m    173\u001b[0m tree \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: ('incompatible input type', <class 'NoneType'>)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/ubuntu/upsc_models_deployment/deployenv/lib/python3.10/site-packages/ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/ubuntu/upsc_models_deployment/deployenv/lib/python3.10/site-packages/traitlets/config/application.py\", line 982, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/ubuntu/upsc_models_deployment/deployenv/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 712, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/home/ubuntu/upsc_models_deployment/deployenv/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 215, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 600, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1860, in _run_once\n",
      "    event_list = self._selector.select(timeout)\n",
      "  File \"/usr/lib/python3.10/selectors.py\", line 469, in select\n",
      "    fd_event_list = self._selector.poll(timeout, max_ev)\n",
      "  File \"/home/ubuntu/upsc_models_deployment/deployenv/lib/python3.10/site-packages/trafilatura/core.py\", line 978, in timeout_handler\n",
      "    raise RuntimeError('unusual file processing time, aborting')\n",
      "RuntimeError: unusual file processing time, aborting\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "url = 'https://www.thehindu.com/news/national/all-party-meet-ahead-of-budget-session-opposition-raises-adani-issue-ysr-congress-calls-for-caste-based-economic-census/article66450221.ec'\n",
    "text, text_act = get_data_url(url)\n",
    "if len(text) > 120:\n",
    "    sentences_all = split_into_sentences(text)\n",
    "    sentences_chunks = get_cuts(text, sentences_all)\n",
    "    pid = 0\n",
    "    for payload in sentences_chunks:\n",
    "        sentence_labels = get_sentence_labels(url,pid,payload,clf,model,sent_no=4)\n",
    "        sentence_keywords = list(set(payload) -  set(sentence_labels.sentences.values))\n",
    "        keyphrases = get_keywords_text(url,pid,sentence_keywords,extractor)\n",
    "        text = ' '.join(payload)\n",
    "        summary = get_summary(url,pid,text, summarizer, th= min(int(len(text)/10),240))\n",
    "        pid = pid+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_ = 'Policy Change Request Effective 05/29/2020 Please remove the PO Box from the insured’s mailing address.  They no longer have it.  The mailing address should read: 4 South Main Street Haydenville, MA  01039 All other aspects of the policy should remain unchanged.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor(str_, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in keyphrases:\n",
    "    print(i,';',inv_map[clf.predict(model.predict_proba([i]))[0]] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.predict_proba(model.predict_proba(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer(text[:5000], max_length= 240, min_length=120, do_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "json.dumps(list(keyphrases))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "__TableName__ = 'prod1_app_data'\n",
    "client  = boto3.client('dynamodb',region_name = 'ap-south-1')\n",
    "DB  = boto3.resource('dynamodb',region_name = 'ap-south-1')\n",
    "table = DB.Table(__TableName__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.hindustantimes.com/india-news/expect-russia-to-be-part-of-all-processes-says-india-on-g20-presidency-101669906231427.html'\n",
    "auth = '1'\n",
    "flag = 1\n",
    "text = sentence_labels[0][0]\n",
    "label_1 = sentence_labels[0][1]\n",
    "label_2 = sentence_labels[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "item = create_item(url,auth, flag,text, label_1,label_2)\n",
    "response = table.put_item(Item  = item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"label_text_2\":{\"M\":{\" the Global South, such as food, fuel and fertilisers,” he said.\":{\"S\":\"society\"},\"However, Prime Minister Narendra Modi told Russian President Vladimir Putin at a meeting in September that today’s era is “not of war”.\":{\"S\":\"international relations\"},\"From time to time, both countries indicate areas of interest or priority that they may be looking at”.\":{\"S\":\"geography\"},\"India said on Thursday it expects Russia to be part of all the processes of G20 as it assumed the presidency of the grouping against the backdrop of persisting differences among its members over the Ukraine war.\":{\"S\":\"defence\"},\"Expect Russia to be part of all processes, says India on G20 presidency India, which began its year-long G20 presidency on Thursday, and Indonesia\":{\"S\":\"geography\"}}},\"label_text_1\":{\"M\":{\" the Global South, such as food, fuel and fertilisers,” he said.\":{\"S\":\"agriculture\"},\"However, Prime Minister Narendra Modi told Russian President Vladimir Putin at a meeting in September that today’s era is “not of war”.\":{\"S\":\"defence\"},\"From time to time, both countries indicate areas of interest or priority that they may be looking at”.\":{\"S\":\"international relations\"},\"India said on Thursday it expects Russia to be part of all the processes of G20 as it assumed the presidency of the grouping against the backdrop of persisting differences among its members over the Ukraine war.\":{\"S\":\"international relations\"},\"Expect Russia to be part of all processes, says India on G20 presidency India, which began its year-long G20 presidency on Thursday, and Indonesia\":{\"S\":\"international relations\"}}}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model.predict_proba(['Expect Russia to be part of all processes, says India on G20 presidency India, which began its year-long G20 presidency on Thursday, and Indonesia'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict_proba(['The decisive moment will be September [2023] when the [G20] summit comes together.'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len('Expect Russia to be part of all processes, says India on G20 presidency India, which began its year-long G20 presidency on Thursday, and Indonesia, the previous president, played a key role in finalising a joint communique at the Bali summit amid deep divisions between Russia and the West')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len('Expect Russia to be part of all processes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([len(i) for i in sentences_final])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([len(i) for i in sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sentences_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s =  'Expect Russia to be part of all processes, says India on G20 presidency India which began its year-long G20 presidency on Thursday '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_comma = s.split(',', 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_comma = np.array([i for i in range(len(s)) if s.startswith(',', i)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmin(np.abs(pos_comma - (len(s) - pos_comma)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_comma[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s[:146]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s[147:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " (len(s) - pos_comma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions_model(text, model, map_):\n",
    "    predictions = model.predict_proba(text.split('.'))\n",
    "    predictions_label = []\n",
    "    for i in predictions:\n",
    "        if np.max(i)> th:\n",
    "            predictions_label.append(np.argmax(predictions[0])) \n",
    "        else:\n",
    "            predictions_label.append(None) \n",
    "    return(text.split('.'), predictions_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(['environment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "keywords = []\n",
    "for sent in text.split('.'):\n",
    "    for i in nlp(sent):\n",
    "        keywords.append(i['word'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "class TextRank4Keyword():\n",
    "    \"\"\"Extract keywords from text\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.d = 0.85 # damping coefficient, usually is .85\n",
    "        self.min_diff = 1e-5 # convergence threshold\n",
    "        self.steps = 10 # iteration steps\n",
    "        self.node_weight = None # save keywords and its weight\n",
    "\n",
    "    \n",
    "    def set_stopwords(self, stopwords):  \n",
    "        \"\"\"Set stop words\"\"\"\n",
    "        for word in STOP_WORDS.union(set(stopwords)):\n",
    "            lexeme = nlp.vocab[word]\n",
    "            lexeme.is_stop = True\n",
    "    \n",
    "    def sentence_segment(self, doc, candidate_pos, lower):\n",
    "        \"\"\"Store those words only in cadidate_pos\"\"\"\n",
    "        sentences = []\n",
    "        for sent in doc.sents:\n",
    "            selected_words = []\n",
    "            for token in sent:\n",
    "                # Store words only with cadidate POS tag\n",
    "                if token.pos_ in candidate_pos and token.is_stop is False:\n",
    "                    if lower is True:\n",
    "                        selected_words.append(token.text.lower())\n",
    "                    else:\n",
    "                        selected_words.append(token.text)\n",
    "            sentences.append(selected_words)\n",
    "        return sentences\n",
    "        \n",
    "    def get_vocab(self, sentences):\n",
    "        \"\"\"Get all tokens\"\"\"\n",
    "        vocab = OrderedDict()\n",
    "        i = 0\n",
    "        for sentence in sentences:\n",
    "            for word in sentence:\n",
    "                if word not in vocab:\n",
    "                    vocab[word] = i\n",
    "                    i += 1\n",
    "        return vocab\n",
    "    \n",
    "    def get_token_pairs(self, window_size, sentences):\n",
    "        \"\"\"Build token_pairs from windows in sentences\"\"\"\n",
    "        token_pairs = list()\n",
    "        for sentence in sentences:\n",
    "            for i, word in enumerate(sentence):\n",
    "                for j in range(i+1, i+window_size):\n",
    "                    if j >= len(sentence):\n",
    "                        break\n",
    "                    pair = (word, sentence[j])\n",
    "                    if pair not in token_pairs:\n",
    "                        token_pairs.append(pair)\n",
    "        return token_pairs\n",
    "        \n",
    "    def symmetrize(self, a):\n",
    "        return a + a.T - np.diag(a.diagonal())\n",
    "    \n",
    "    def get_matrix(self, vocab, token_pairs):\n",
    "        \"\"\"Get normalized matrix\"\"\"\n",
    "        # Build matrix\n",
    "        vocab_size = len(vocab)\n",
    "        g = np.zeros((vocab_size, vocab_size), dtype='float')\n",
    "        for word1, word2 in token_pairs:\n",
    "            i, j = vocab[word1], vocab[word2]\n",
    "            g[i][j] = 1\n",
    "            \n",
    "        # Get Symmeric matrix\n",
    "        g = self.symmetrize(g)\n",
    "        \n",
    "        # Normalize matrix by column\n",
    "        norm = np.sum(g, axis=0)\n",
    "        g_norm = np.divide(g, norm, where=norm!=0) # this is ignore the 0 element in norm\n",
    "        \n",
    "        return g_norm\n",
    "\n",
    "    \n",
    "    def get_keywords(self, number=10):\n",
    "        \"\"\"Print top number keywords\"\"\"\n",
    "        node_weight = OrderedDict(sorted(self.node_weight.items(), key=lambda t: t[1], reverse=True))\n",
    "        for i, (key, value) in enumerate(node_weight.items()):\n",
    "            print(key + ' - ' + str(value))\n",
    "            if i > number:\n",
    "                break\n",
    "        \n",
    "        \n",
    "    def analyze(self, text, \n",
    "                candidate_pos=['NOUN', 'PROPN'], \n",
    "                window_size=4, lower=False, stopwords=list()):\n",
    "        \"\"\"Main function to analyze text\"\"\"\n",
    "        \n",
    "        # Set stop words\n",
    "        self.set_stopwords(stopwords)\n",
    "        \n",
    "        # Pare text by spaCy\n",
    "        doc = nlp(text)\n",
    "        \n",
    "        # Filter sentences\n",
    "        sentences = self.sentence_segment(doc, candidate_pos, lower) # list of list of words\n",
    "        \n",
    "        # Build vocabulary\n",
    "        vocab = self.get_vocab(sentences)\n",
    "        \n",
    "        # Get token_pairs from windows\n",
    "        token_pairs = self.get_token_pairs(window_size, sentences)\n",
    "        \n",
    "        # Get normalized matrix\n",
    "        g = self.get_matrix(vocab, token_pairs)\n",
    "        \n",
    "        # Initionlization for weight(pagerank value)\n",
    "        pr = np.array([1] * len(vocab))\n",
    "        \n",
    "        # Iteration\n",
    "        previous_pr = 0\n",
    "        for epoch in range(self.steps):\n",
    "            pr = (1-self.d) + self.d * np.dot(g, pr)\n",
    "            if abs(previous_pr - sum(pr))  < self.min_diff:\n",
    "                break\n",
    "            else:\n",
    "                previous_pr = sum(pr)\n",
    "\n",
    "        # Get weight for each node\n",
    "        node_weight = dict()\n",
    "        for word, index in vocab.items():\n",
    "            node_weight[word] = pr[index]\n",
    "        \n",
    "        self.node_weight = node_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr4w = TextRank4Keyword()\n",
    "tr4w.analyze(text, candidate_pos = ['NOUN','PROPN', 'VERB'], window_size=10, lower=False)\n",
    "tr4w.get_keywords(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in nlp(df.headings.values[14]):\n",
    "    print(i['word'], get_label(i['word'],model_sim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_label('apple',model_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.headings.values[14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('data/training_data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install trafilatura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_extracted.replace('\\n',' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENTRYPOINT [\"uvicorn main:app --host\", \"0.0.0.0\", \"--port\", \"80\"]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('deployenv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9d43577eea919236b69ebd0f8998005d41bfb2309937fa01b8dc5c2f454a02cb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
